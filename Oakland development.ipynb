{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development history of a neighborhood in Oakland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growing up in a small town on the East Coast, I knew the person who built the first house I lived in.  Not personally, mind you, but I knew that he was the brother of the father of our next-door-neighbors (who were themselves quite old when I was a child).  Then I began to move around: first for college, then for graduate school, then to the Bay Area.  When I recently moved to Oakland and started putting down roots, I was missing the same kind of connection to my new neighborhood.  So I got a copy of Beth Bagwell's book [*Oakland The Story of a City*](http://blog.ouroakland.net/2012/05/oakland-story-of-city.html), and that filled in some broad brush strokes for me: first the Muwekma Ohlone villages and shell mounds, then the Spanish land grants to the great rancheros (Peralta and his sons), then the gold rush and the land grab by greedy settlers, then the development of downtown and gradual spreading of the city boundary with the arrival of mass transit.  Then my neighborhood was built.\n",
    "\n",
    "Still left unanswered was when was my city block constructed?  Sure, one can find old maps that list the development parcels, but what did the neighborhood look like going up?  Was it built all at once in the roaring 20s, or piece-meal, one-farmhouse here, another there, as the land slowly became subdivided and the orchards and oak trees cut down?  This is a narrow question, and limited to what can be seen now.  It may overlook structures that burned down or were razed, or how the land was used by the Peraltas for many decades, and the Ohlone for many centuries before them.  But the further back one looks, the fainter the traces.  Or, said another way: you have to start somewhere.\n",
    "\n",
    "So I decided to answer the simplest question I could: when was each house built?  This required some information that shouldn't be too hard to find: a list of houses, where each one is, and when it was built.  I'll describe the process of analyzing this data as I go along, complete with snippets in case others want to replicate this info.  Standalone scripts can be found in this repository as well, as indicated in the first comments.\n",
    "\n",
    "To skip right to the analysis, scroll down until you see a big-scale colored map.\n",
    "\n",
    "**REVISE this section!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data sets made available after [CodeForOakland](http://codeforoakland.org/data-sets/#oakland1) have been critical for this project.  Initially, I found a GeoJSON file for containing parcel info for the county of Alameda, which includes Oakland and extends north-to-south from Berkeley to Fremont, and east-to-west from the Bay to beyond Livermore.  This was a huge file, couldn't be processed in memory, contained about 10 times the amount of data required, and lacked street addresses.  Several days later, I found a shapefile of all parcels in Oakland, from around 2011, that conveniently fit into memory.  Still, if I want to go back and look at the info for Piedmont (or neighboring cities), the data's there.\n",
    "\n",
    "This is the info for each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels 105351\n",
      "\n",
      "Second parcel:\n",
      "{'geometry': {'coordinates': [[(-122.244024425766, 37.86867162821933),\n",
      "                               (-122.24398667467038, 37.86866162294282),\n",
      "                               (-122.24399852865695, 37.868349112878676),\n",
      "                               (-122.24401355395213, 37.86835277617628),\n",
      "                               (-122.24404457097216, 37.868359870908144),\n",
      "                               (-122.2440757393451, 37.86836653553029),\n",
      "                               (-122.24408444864746, 37.86836826921976),\n",
      "                               (-122.24407290434733, 37.86867278199862),\n",
      "                               (-122.244024425766, 37.86867162821933)]],\n",
      "              'type': 'Polygon'},\n",
      " 'id': '1',\n",
      " 'properties': OrderedDict([('OBJECTID', 23),\n",
      "                            ('ADDR_HN', None),\n",
      "                            ('ADDR_PD', None),\n",
      "                            ('ADDR_SN', 'DWIGHT'),\n",
      "                            ('ADDR_ST', 'WAY'),\n",
      "                            ('OBJECTID_1', 116),\n",
      "                            ('APN', '048H770309700'),\n",
      "                            ('ADDRNUM', 0),\n",
      "                            ('STNAME', 'DWIGHT WY'),\n",
      "                            ('UNIT', None),\n",
      "                            ('ZIP', 94704),\n",
      "                            ('DATASRC', 'ASSR_20031023'),\n",
      "                            ('ORIGKEY', '048H770309700'),\n",
      "                            ('ADDRESS_AP', '048H770309700'),\n",
      "                            ('APNSRC', None),\n",
      "                            ('ADDRNUM_TX', None),\n",
      "                            ('SHAPE_AREA', 2830.19306092),\n",
      "                            ('SHAPE_LEN', 276.033464863)]),\n",
      " 'type': 'Feature'}\n"
     ]
    }
   ],
   "source": [
    "# insert code for reading shape file, print an entry\n",
    "import fiona\n",
    "import pprint\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "source = fiona.open(baseFile + '.shp')\n",
    "print('Number of parcels: %d\\n' % len(source))\n",
    "print('Second parcel:')\n",
    "source.next()    # first entry has a lot of coordinates, so skip\n",
    "pprint.pprint(source.next())\n",
    "\n",
    "source.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this database has a field for the street address, though the specific example above lacks this information ('None').  I'll catch this kind of error below.  If the street address wasn't present, it could be found by reverse geocoding the location (from Google's API, for instance).  Also, there are 105,000 entries all told.\n",
    "\n",
    "The construction history exists in Zillow's database, so the dataset was completed by calling Zillow's API for each property.  The one catch is that Zillow's free API key allows only 1000 queries per day.  Because I'm mostly interested in one neighborhood, I can approach this rate limit in a smart way by working outward from a central point.  I'll chooose the center location to be a landmark in my neighborhood of Cleveland Heights: the Armenian Church on the top of the hill at McKinley and Spruce.  At this rate, it will take 105 days (~3.5 months) to process all the data for Oakland; all the surrounding neighborhoods will take a couple of weeks.\n",
    "\n",
    "To process the parcels radially, the shapefile array was rewritten as a dictionary with the key being the distance between the landmark and the centroid of each parcel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels in dict: 97718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "import shapely.geometry as shp\n",
    "import pickle \n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "center = (37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "\n",
    "data_raw = {}\n",
    "data_duplicates = {}\n",
    "\n",
    "with fiona.drivers():           # Register format drivers with a context manager\n",
    "\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "       \n",
    "        for f in source :\n",
    "            if 'geometry' not in f:\n",
    "                print('No geometry key in entry {}'.format(f))\n",
    "            c = shp.shape(f['geometry']).centroid\n",
    "            p = (c.y, c.x)\n",
    "\n",
    "            d = round(distance(p, center).m * 10**6)/10**6        # round to micrometers\n",
    "            f['centroid'] = p            \n",
    "\n",
    "            if d in data_raw :\n",
    "                if d in data_duplicates :   \n",
    "                    data_duplicates[d].append(f)      # add to list in existing dictionary key\n",
    "                else :\n",
    "                    data_duplicates[d] = [data_raw[d], f]      # create list in existing dictionary key\n",
    "            else :\n",
    "                data_raw[d] = f\n",
    "print('Number of parcels in dict: %d\\n' % len(data_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, computing the centroid is unnecessary for radial processing - the first coordinate of the parcel shape would suffice - but this info may be useful later on.  Also, the amount of time it adds to this processing is negligible.\n",
    "\n",
    "Notice that there are now about 97000 entries, about 8000 less than the original file.  It turns out that these are duplicate entries, such as condominiums, that share the same street address and coordinates but have different assessor parcel numbers (APNs).  Doing the math confirms that this explains all duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate parcels: 7633\n"
     ]
    }
   ],
   "source": [
    "duplicateCount = 0\n",
    "for (key, value) in data_duplicates.items() :\n",
    "    duplicateCount += len(value)\n",
    "print('Total number of duplicate parcels: %d' % (duplicateCount-len(data_duplicates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And note that 7633+97718 = 105351, which was the original number of entries in the shapefile.\n",
    "\n",
    "Interestingly, the micrometer precision in the distance key is necessary to distinguish parcels: rounding only to millimeters results in clashes between different street addresses.  Given the number of parcels at a certain radius once the radius gets large, this isn't terribly surprising, but still an nice example about the importance of precision and probability.\n",
    "\n",
    "Now the cleaned dataset is saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/OaklandParcels_inProcess.pkl', 'wb') as datafile :\n",
    "    a = pickle.Pickler(datafile)\n",
    "    compressed = {}\n",
    "    compressed['data_raw'] = data_raw\n",
    "    compressed['data_queried'] = {}\n",
    "    compressed['data_errors'] = []\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is processed starting with the smallest key and working outwards: each entry is first sent to the zillow API, then popped from the input dictionary (data_raw) and placed in the output dictionary (data_queried) if the response is valid.  If the response is invalid, it is placed into an error dictionary (data_errors) for later processing.  There's also a rate limit of 10 queries/second, so a timer around the loop limits the rate.  It runs at 5 queries/second just to be nice to Zillow's server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CreateParcelDatabase.py\n",
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Zillow variables keys\n",
    "with open('../private/API_keys.pkl', 'rb') as datafile:\n",
    "    zid = pickle.load(datafile)             # API key\n",
    "zurl = 'http://www.zillow.com/webservice/GetDeepSearchResults.htm?'\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'\n",
    "radius = 1000            # only process parcels within this radius - used for debugging\n",
    "numToProcess = 1000      # zillow API limits to 1000 queries per day\n",
    "\n",
    "# load data structures\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "data_errors = compressed['data_errors']\n",
    "del compressed\n",
    "\n",
    "# sort keys by distance from closest to furthest\n",
    "sortedKeys = [k for k in sorted(data_raw) if k < radius]\n",
    "\n",
    "for (i, key) in zip(range(numToProcess), sortedKeys) :\n",
    "    startT = time.time()            # set up timer to keep requests under 10/s\n",
    "\n",
    "    try :\n",
    "        # read in address details from input dictionary\n",
    "        zp = {'address' : '{} {} {}'.format(data_raw[key]['properties']['ADDR_HN'],\n",
    "                      data_raw[key]['properties']['ADDR_SN'],\n",
    "                      data_raw[key]['properties']['ADDR_ST']),\n",
    "              'citystatezip' : 'Oakland, CA ' + str(data_raw[key]['properties']['ZIP']),\n",
    "              'zws-id' : zid}\n",
    "        r = requests.get(zurl, params=zp)\n",
    "        r_dict = xmltodict.parse(r.text)['SearchResults:searchresults']\n",
    "\n",
    "        if r_dict['message']['code'] == '0' :       # valid response?\n",
    "            r_dict = r_dict['response']['results']['result']\n",
    "            \n",
    "            # in case the response is a list of multiple (similar) entries, take the first one\n",
    "            if type(r_dict)==list :\n",
    "                r_dict = r_dict[0]\n",
    "            # prune extraneous fields\n",
    "            r_dict.pop('links')\n",
    "            r_dict.pop('zestimate')\n",
    "            r_dict.pop('localRealEstate')\n",
    "            data_raw[key]['zillow'] = r_dict\n",
    "            # transfer  to output dictionary\n",
    "            data_queried[key] = data_raw.pop(key)\n",
    "            \n",
    "        else :\n",
    "            print('For request {}, zillow code is {}. Here''s the record:'.format(\n",
    "                    zp['address'], r_dict['message']['code']))\n",
    "            print(r_dict)\n",
    "            print('-'*60)\n",
    "            # transfer info to error dictionary for offline analysis\n",
    "            data_errors.append({'key': key, 'value': data_raw.pop(key), \n",
    "                                  'zillow': r_dict, 'source': 'zillow'})\n",
    "    except Exception as exc:\n",
    "        print('Unspecified error: {}'.format(exc))\n",
    "        data_errors.append({'key': key, 'value': data_raw.pop(key),\n",
    "                              'source': 'exception'})\n",
    "            \n",
    "    # log status\n",
    "    print(i, ' ', zp['address'])\n",
    "\n",
    "    endT = time.time()\n",
    "    if endT - startT < 0.2 :\n",
    "        time.sleep(0.2 - (endT-startT))     # rate limit to 5 calls per second\n",
    "\n",
    "# save dictionaries back to disk\n",
    "compressed = {}\n",
    "compressed['data_raw'] = data_raw\n",
    "compressed['data_queried'] = data_queried\n",
    "compressed['data_errors'] = data_errors\n",
    "with open(inProcessFile, 'wb') as fid :\n",
    "    a = pickle.Pickler(fid)\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common error was code 508, 'no exact match found for input address'.  This was primarily caused by the address not being in the Zillow database, such as for commercial buildings, churches, schools, etc.  But this was also caused by invalid address, such as a parcel without a street address ('None MacArthur Blvd'), which was the case for parks, municipal land, and Lake Merrit.  The error rate was about 5%, or 1 in 20.\n",
    "\n",
    "Before mapping the data, it needs to be written back to a shapefile so it can be easily processed.  Even though shapefiles are an old format, they are pretty efficient for this kind of processing - much more so than the GeoJSON format.  When doing so, the schema from the original file needs to be modified to include the new data ('yearBuilt'); extra fields from the compelation stage are jettisoned to speed up processing.  Finally, because this shapefile uses an ordered dictionary of 'properties', the data fields need to be rearranged to match the schema's order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SaveParcelDictionaryAsShapefile.py\n",
    "import fiona\n",
    "import pickle\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'         # data source\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "del compressed\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'       # source of shape info in dictionary\n",
    "outputFileName = 'Oakland_parcels_queried'\n",
    "radius = 2000                                   # in m\n",
    "\n",
    "# create output directory if it doesn't exist yet\n",
    "if os.path.isdir('data/' + outputFileName) is False :\n",
    "    os.makedirs('data/' + outputFileName)\n",
    "outputFile = 'data/' + outputFileName + '/' + outputFileName + '.shp'\n",
    "\n",
    "# Register format drivers with a context manager\n",
    "with fiona.drivers():\n",
    "    # get schema from original file\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "        meta = source.meta\n",
    "        \n",
    "    # add new fields to schema file\n",
    "    meta['schema']['centroid'] = ('float:19:11', 'float:19:11')\n",
    "    meta['schema']['id'] = 'float:19'\n",
    "    meta['schema']['type'] = 'str:50'\n",
    "    meta['schema']['yearBuilt'] = 'float:10'\n",
    "    meta['schema']['properties']['YEARBUILT'] = 'int:6'\n",
    "    meta['schema']['properties'] = OrderedDict(meta['schema']['properties'])\n",
    "    schemaOrder = meta['schema']['properties']\n",
    "\n",
    "    with fiona.open(outputFile, 'w', **meta) as sink:\n",
    "        for (i,f) in enumerate(data_queried) :\n",
    "            if f <= radius :   \n",
    "                if 'yearBuilt' in data_queried[f]['zillow'] :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = data_queried[f]['zillow']['yearBuilt']\n",
    "                    data_queried[f].pop('zillow')\n",
    "                else :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = np.nan\n",
    "                # reorder dictionary to match schema order\n",
    "                data_queried[f]['properties'] = OrderedDict(\n",
    "                        (k, data_queried[f]['properties'][k]) for k in schemaOrder)           \n",
    "                sink.write(data_queried[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kind of map used to show this data is a choropleth map, which maps a physical quantity (year built) onto a spatial extent by using some kind of shading.  This is a quick way to show what spatial patterns might require more analysis.  While Python has no straightforward way to do this, all the tools required are free and there's extensive support and code examples online.\n",
    "\n",
    "The general overview is to first choose a projection grid to map the round world onto.  This converts the (longitude, latitude) pairs to (x, y) pairs in the coordinate system of the projection.  On top of this, the parcel polygons are drawn and colored.  First I'll list functions to make colorbars from [Sensitive Cities](http://sensitivecities.com/so-youd-like-to-make-a-map-using-python-EN.html#.V2hnJa4tVVz), then will come the mapping routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convenience functions for working with colour ramps and bars\n",
    "def colorbar_index(ncolors, cmap, labels=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This is a convenience function to stop you making off-by-one errors\n",
    "    Takes a standard colour ramp, and discretizes it,\n",
    "    then draws a colour bar with correctly aligned labels\n",
    "    \"\"\"\n",
    "    cmap = cmap_discretize(cmap, ncolors)\n",
    "    mappable = cm.ScalarMappable(cmap=cmap)\n",
    "    mappable.set_array([])\n",
    "    mappable.set_clim(-0.5, ncolors+0.5)\n",
    "    colorbar = plt.colorbar(mappable, **kwargs)\n",
    "    colorbar.set_ticks(np.linspace(0, ncolors, ncolors))\n",
    "    colorbar.set_ticklabels(range(ncolors))\n",
    "    if labels:\n",
    "        colorbar.set_ticklabels(labels)\n",
    "    return colorbar\n",
    "\n",
    "def cmap_discretize(cmap, N):\n",
    "    \"\"\"\n",
    "    Return a discrete colormap from the continuous colormap cmap.\n",
    "\n",
    "        cmap: colormap instance, eg. cm.jet. \n",
    "        N: number of colors.\n",
    "\n",
    "    Example\n",
    "        x = resize(arange(100), (5,100))\n",
    "        djet = cmap_discretize(cm.jet, 5)\n",
    "        imshow(x, cmap=djet)\n",
    "\n",
    "    \"\"\"\n",
    "    if type(cmap) == str:\n",
    "        cmap = get_cmap(cmap)\n",
    "    colors_i = np.concatenate((np.linspace(0, 1., N), (0., 0., 0., 0.)))\n",
    "    colors_rgba = cmap(colors_i)\n",
    "    indices = np.linspace(0, 1., N + 1)\n",
    "    cdict = {}\n",
    "    for ki, key in enumerate(('red', 'green', 'blue')):\n",
    "        cdict[key] = [(indices[i], colors_rgba[i - 1, ki], colors_rgba[i, ki]) for i in xrange(N + 1)]\n",
    "    return matplotlib.colors.LinearSegmentedColormap(cmap.name + \"_%d\" % N, cdict, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DrawParcelChoropleth.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "from descartes import PolygonPatch\n",
    "from itertools import chain\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "\n",
    "# shapefile database\n",
    "baseFile = 'data/Oakland_parcels_queried/Oakland_parcels_queried'\n",
    "\n",
    "center = geopy.Point(37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "radius = 0.5                           # in km\n",
    "ur = distance(kilometers=radius*2**0.5).destination(center, +45)\n",
    "ll = distance(kilometers=radius*2**0.5).destination(center, -135)\n",
    "ur = (ur.longitude, ur.latitude)\n",
    "ll = (ll.longitude, ll.latitude)\n",
    "extra = 0.01           # padding for edges\n",
    "coords = list(chain(ll, ur))\n",
    "w, h = coords[2] - coords[0], coords[3] - coords[1]\n",
    "\n",
    "m = Basemap(\n",
    "    projection='tmerc',\n",
    "    lon_0=-122.,\n",
    "    lat_0=37.,\n",
    "    ellps = 'WGS84',\n",
    "    llcrnrlon=coords[0] - extra * w,\n",
    "    llcrnrlat=coords[1] - extra * h,\n",
    "    urcrnrlon=coords[2] + extra * w,\n",
    "    urcrnrlat=coords[3] + extra * h,\n",
    "    lat_ts=0,\n",
    "    resolution='i',\n",
    "    suppress_ticks=True)\n",
    "\n",
    "m.readshapefile(\n",
    "    baseFile,\n",
    "    'oakland',\n",
    "    color='blue',\n",
    "    zorder=2)\n",
    "  \n",
    "# set up a map dataframe\n",
    "df_map = pd.DataFrame({\n",
    "    'poly': [Polygon(xy) for xy in m.oakland],\n",
    "    'id': [obj['OBJECTID'] for obj in m.oakland_info],\n",
    "    'zip': [obj['ZIP'] for obj in m.oakland_info],\n",
    "    'yearBuilt': [obj['YEARBUILT'] for obj in m.oakland_info]})\n",
    "\n",
    "# Create projection view as a polygon to filter shapes\n",
    "window = [ll, (ll[0], ur[1]), ur, (ur[0], ll[1]), ll]\n",
    "window = list(zip( *m(*list(zip(*window))) ))\n",
    "window_map = pd.DataFrame({'poly': [Polygon(window)]})\n",
    "window_polygon = prep(MultiPolygon(list(window_map['poly'].values)))\n",
    "\n",
    "# Remove any shapes that are outside the map window\n",
    "df_map = df_map[ [window_polygon.intersects(i) for i in df_map.poly] ]\n",
    "\n",
    "# draw tract patches from polygons\n",
    "df_map['patches'] = df_map['poly'].map(lambda x: PolygonPatch(\n",
    "    x,\n",
    "    ec='#787878', lw=.25, alpha=.9,\n",
    "    zorder=4))\n",
    "\n",
    "# create colormap based on year built\n",
    "cmap_range = (1890, 1930)\n",
    "ncolors = 8\n",
    "yearBuilt_bins = np.linspace(min(cmap_range), max(cmap_range), ncolors+1)\n",
    "cmap = matplotlib.cm.coolwarm\n",
    "cmap.set_bad(color='white')       # if yearBuilt is nan\n",
    "norm = matplotlib.colors.BoundaryNorm(yearBuilt_bins, ncolors)\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
    "\n",
    "# plot parcels by adding the PatchCollection to the axes instance\n",
    "pc = PatchCollection(df_map['patches'].values, match_original=True)\n",
    "pc.set_facecolor(cmap(norm(df_map.yearBuilt)/ncolors));\n",
    "ax.add_collection(pc)\n",
    "\n",
    "# create labels for colorbar\n",
    "yearBuilt_labels = ['%.0f-%.0f' % (yearBuilt_bins[i], yearBuilt_bins[i+1])\n",
    "                        for i in range(ncolors)]\n",
    "yearBuilt_labels.append('>%.0f' % yearBuilt_bins[-1])\n",
    "\n",
    "cb = colorbar_index(ncolors=ncolors+1, cmap=cmap, shrink=0.5, labels=yearBuilt_labels)\n",
    "cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "# Draw a map scale\n",
    "m.drawmapscale(\n",
    "    coords[0] + w * 0.5, coords[1] + h * 0.1,\n",
    "    coords[0], coords[1],\n",
    "    radius/2*1000,   # length\n",
    "    barstyle='fancy', labelstyle='simple',\n",
    "    units = 'm',\n",
    "#    format='%.2f',\n",
    "    fillcolor1='w', fillcolor2='#555555',\n",
    "    fontcolor='#555555',\n",
    "    zorder=4)\n",
    "plt.title(\"Parcels in Oakland, labeled by year built\")\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(7.22, 5.25)  \n",
    "plt.savefig('data/Oakland_temp.png', dpi=300, alpha=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/data/Oakland_500m_1890to1930.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's easy to see a couple of trends in this small scale map:\n",
    "- Although roads aren't labeled, they are clearly visible in the negative space.  The big diagonal swoosh is I-580, and the almost horizontal road spanning the bottom is Park Blvd.  Even some pedestrian pathways, a common occurance in the hilly East Bay, are visible as thin lines between houses in the upper right.\n",
    "- Some parcels aren't labeled at all, not even with parcel boundaries.  These are not in Zillow's database, such as schools, churches, commercial property, and parks.  The big empty space on the right side is Oakland High School.\n",
    "- Some parcels have boundaries but are colored white.  This indicates they were either built after this date span or have no valid date of construction - overflow or invalid data, in other words.\n",
    "\n",
    "With that in mind, here are two larger scale maps containing the 6000 parcels closest to the centerpoint, for two slightly overlapping time scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/data/Oakland_1300m_1880to1920.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/data/Oakland_1300m_1920to1960.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these larger scale maps, a couple trends can be seen.  In the late 19th Century, or the Victorian Era, houses were built to the south and east of Lake Merritt, primarily in the Clinton and Bella Vista neighborhoods, with a few between Lake Merrit and Cleveland Heights.  These houses tended to group in small numbers and be separated by a block or two: perhaps this area was still used for agriculture then.  In any case, this area was called Brooklyn and was separated from downtown Oakland by a toll bridge then.\n",
    "\n",
    "The first two decades of the 20th C saw the edges of development spreading outward.  The development was strongest in the neighborhoods of Lakeshore, Cleveland Heights, and the upper reaches of Trestle Glen (to the SE of the label).  These areas aligned closely with the building of the streetcar system, which by then ran through all these areas.\n",
    "\n",
    "Just about the only area to see complete development was Bella Vista, likely because its proximity to the Arbor Villa estate of Francis \"Borax\" Smith made it a very desirable location.   Oddly, Haddon Hill saw only a few houses being built before 1920 - perhaps this hill that overlooks Lake Merritt smelled to strongly, for the lake (truly a tidal inlet then) was quite polluted.  The valley of Trestle Glen was almost ignored during this period: the train trestle that lent the name was torn down around 1906, but very few houses were built until later.\n",
    "\n",
    "Then in the 1920s most of these neighborhoods were almost completely developed - reaching into 90% coverage.  The big exceptions are the Clinton area, which saw only 70-80% coverage, Arbor Villa, and the western slope of Haddon Hill.  Arbor Villa wasn't built until Francis Smith went bankrupt in the early 1930s and had to sell off his estate that spanned 5 city blocks.  The only thing remaining - sadly, his mansion was razed - is a row of palm trees on the southern edge that stretches 3 blocks long.  It wasn't until the Great Depression was truly over in 1940 that this area was built in, as you can see in the plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/data/Oakland_ArborVilla.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the 1950s, this area had been almost entirely filled in.  Given the quality of the dataset - to be discussed shortly - it's hard to make statements about specific parcels, which were the only ones built after this time.  An interesting question is how many were rebuilt because of catastrophe (fire or the 1989 Loma Prieta earthquake) versus changing use (converting a single family home to a multi-unit building, or residential to commercial).\n",
    "\n",
    "So my take-away from this analysis is that houses in these districts were not built in large tracts at the same time, as suburbs were in the 1950s, but rather piece-meal over a span of 5-15 years.  But I should qualify this slightly, since no dataset is perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that this dataset is from 2011 or earlier, and does not include any structures that have been razed in the past (or previous parcels that have been subsequently subdivided).  Arbor Villa is the most famous example, but there were surely others.  Refining the dataset to include this data would be a substantial research project in itself (and would probably involve close reading of the Sanborn maps created for insurance quotes in the 19th and early 20th centuries, though they only came out every 5-10 years).\n",
    "\n",
    "How accurate is this data in the first place?  The Zillow information mostly comes from county sources - one can explore this on a property-by-property basis on their website.  Comparing a few cases with historical sources such as the [Oakland Wiki](https://localwiki.org/oakland \"Oakland - LocalWiki\") gives a sense of accuracy, or at least a closer approximation.  Here are some examples:\n",
    "\n",
    "- 2901 Park Blvd (corner of Park and McKinley) was built around 1912 (as part of the [Mary Smith Home for Friendless Girls](https://localwiki.org/oakland/Mary_Smith_Home_for_Friendless_Girls)), while Zillow puts it at 1930.\n",
    "- [1047 Bella Vista](https://oaklandwiki.org/Fenton_Home_Orphanage) was built in 1892, 18 years before Zillow claims it was.  (An interesting aside: Susan Fenton, the sister of the woman who founded Fenton's Creamery, which is still operating on Piedmont St and as beloved as ever, founded a Home for Destitue Children here in 1925.)\n",
    "- [The Kaiser house](https://oaklandwiki.org/Kaiser_House) at 664 Haddon Road, where Henry Kaiser lived between 1925 and the mid 1940s, was built in 1924.  Zillow cites a date of 1925.\n",
    "\n",
    "More historical citations can be found in \"An Architectural Guidebook to San Francisco and the Bay Area\", by Susan Dinkelspiel Cerny (2007), online at https://books.google.com/books?id=FkVQx6MWa8MC&lpg=RA2-PA120&ots=OANNWFMFSG&dq=%221047%20bella%20vista%22%20oakland&pg=RA2-PA120#v=onepage&q=%221047%20bella%20vista%22%20oakland&f=false\n",
    "\n",
    "Based on this comparison, it seems like houses older than around 1920 are less likely to be accurately labeled in Zillow's archives than more recent houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several places to take this analysis:\n",
    "- Outward geographically, first to all of Oakland but also to surrounding communities.  Most prominent would be the surrounded community of Piedmont, which never incorporated into Oakland and is just barely visible as the straight edge at the top of the larger maps.  In particular, it would be interesting to look at the flats of Berkeley, Oakland, and Emeryville that were developed by people displaced by the 1906 earthquake in San Francisco.\n",
    "- Extend this to different attributes of the data.  The Zillow query returns several other interesting fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('zpid', '24763478'),\n",
       "             ('address',\n",
       "              OrderedDict([('street', '684 Spruce St'),\n",
       "                           ('zipcode', '94610'),\n",
       "                           ('city', 'Oakland'),\n",
       "                           ('state', 'CA'),\n",
       "                           ('latitude', '37.806119'),\n",
       "                           ('longitude', '-122.23984')])),\n",
       "             ('FIPScounty', '6001'),\n",
       "             ('useCode', 'MultiFamily2To4'),\n",
       "             ('taxAssessmentYear', '2015'),\n",
       "             ('taxAssessment', '204567.0'),\n",
       "             ('yearBuilt', '1910'),\n",
       "             ('lotSizeSqFt', '4400'),\n",
       "             ('finishedSqFt', '700'),\n",
       "             ('bathrooms', '1.0'),\n",
       "             ('bedrooms', '2'),\n",
       "             ('totalRooms', '8'),\n",
       "             ('lastSoldDate', '02/22/1996'),\n",
       "             ('lastSoldPrice',\n",
       "              OrderedDict([('@currency', 'USD'), ('#text', '148000')]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_queried[32.873555]['zillow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd expect many of these fields to show interesting patterns on this map.  The useCode would show which areas have most rental units compared to single-family homes, and would likely give a good sense of (residential) building height in each area.\n",
    "\n",
    "Total rooms and finished square feet may also be good proxies for how desirable the house was when built: presumably, larger houses were built for wealthier families.  (Of course, these numbers will reflect additions and renovations in the time since, and may not be as reliable as a result).\n",
    "\n",
    "Obviously, assessed value is a crucial variability that I have for the most part ignored because there's already a large industry devoted to that question.\n",
    "\n",
    "In order to follow up the comment about hill-tops (in general) being developed first, the data set would also need to be expanded to get elevation information.  This would then need to be filtered to find local maxima, slopes, and local minima.  Based on the results here, I can easily imagine a weak but discernable correlation ($r=0.2-0.5$) between year built and location type, though I'm not sure this trend would be also be valid for neighborhoods such as the Oakland Hills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
