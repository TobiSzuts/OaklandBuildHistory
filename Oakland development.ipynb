{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development history of a neighborhood in Oakland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was curious how my neighborhood in Oakland, Cleveland Heights, was developed: which house was built first?  Which last?  How did this pattern fit with how the city was developed.  In particular, how much of that history can be read now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find that data for this, I needed to know when each house was built, where each house was, and a list of houses.  Thanks in large part to the great collection of Oakland data at XXURLXX.  Initially, I found a GeoJSON file for containing parcel info for the county of Alameda, which includes Oakland and extends from Berkeley to Fremont north-to-south, and from the Bay to beyond Livermore east-to-west.  This was a huge file, and couldn't be processed in memory.  Several days later, I found parcel info for the City of Oakland from 20XX that conveniently fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels 105351\n",
      "\n",
      "Second parcel:\n",
      "{'geometry': {'coordinates': [[(-122.244024425766, 37.86867162821933),\n",
      "                               (-122.24398667467038, 37.86866162294282),\n",
      "                               (-122.24399852865695, 37.868349112878676),\n",
      "                               (-122.24401355395213, 37.86835277617628),\n",
      "                               (-122.24404457097216, 37.868359870908144),\n",
      "                               (-122.2440757393451, 37.86836653553029),\n",
      "                               (-122.24408444864746, 37.86836826921976),\n",
      "                               (-122.24407290434733, 37.86867278199862),\n",
      "                               (-122.244024425766, 37.86867162821933)]],\n",
      "              'type': 'Polygon'},\n",
      " 'id': '1',\n",
      " 'properties': OrderedDict([('OBJECTID', 23),\n",
      "                            ('ADDR_HN', None),\n",
      "                            ('ADDR_PD', None),\n",
      "                            ('ADDR_SN', 'DWIGHT'),\n",
      "                            ('ADDR_ST', 'WAY'),\n",
      "                            ('OBJECTID_1', 116),\n",
      "                            ('APN', '048H770309700'),\n",
      "                            ('ADDRNUM', 0),\n",
      "                            ('STNAME', 'DWIGHT WY'),\n",
      "                            ('UNIT', None),\n",
      "                            ('ZIP', 94704),\n",
      "                            ('DATASRC', 'ASSR_20031023'),\n",
      "                            ('ORIGKEY', '048H770309700'),\n",
      "                            ('ADDRESS_AP', '048H770309700'),\n",
      "                            ('APNSRC', None),\n",
      "                            ('ADDRNUM_TX', None),\n",
      "                            ('SHAPE_AREA', 2830.19306092),\n",
      "                            ('SHAPE_LEN', 276.033464863)]),\n",
      " 'type': 'Feature'}\n"
     ]
    }
   ],
   "source": [
    "# insert code for reading shape file, print an entry\n",
    "import fiona\n",
    "import pprint\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "source = fiona.open(baseFile + '.shp')\n",
    "print('Number of parcels: %d\\n' % len(source))\n",
    "print('Second parcel:')\n",
    "source.next()    # first entry has a lot of coordinates, so skip\n",
    "pprint.pprint(source.next())\n",
    "\n",
    "source.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, each entry also includes fields for the street address, though this information is missing here.  I'll catch these errors below.  If the street address wasn't present, it another call to reverse geocode the location (from Google, for instance) would have been required.  Note that there are 105,000 entries in all.\n",
    "\n",
    "The construction history exists on Zillow, so this database was completed by repeat API calls.  The free Zillow API key that I have only allows 1000 queries per day.  Because I'm mostly interested in one neighborhood, I can approach this rate limit in a smart way by working outward from a central point.  I'll chooose the center location to be the Armenian Church on the top of the hill at McKinley and Spruce, since it's a big landmark close that's pretty central anyways.  (Another choice would have been the elmenetary school 3 blocks away.)  It will take 105 days (~3.5 months) to process all the data for Oakland.\n",
    "\n",
    "To do this radial processing, the shapefile array was rewritten as a dictionary with the key being the distance from the landmark.  The distance was computed from the centroid of the parcel shape.  Technically, computing the centroid is unnecessary for radial proessing - the first coordinate of the parcel shape would suffice - but this may be useful later on.  Also, it doesn't take much time on such a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels in dict: 97718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "import shapely.geometry as shp\n",
    "import pickle \n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "center = (37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "\n",
    "data_raw = {}\n",
    "data_duplicates = {}\n",
    "\n",
    "with fiona.drivers():           # Register format drivers with a context manager\n",
    "\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "       \n",
    "        for f in source :\n",
    "            if 'geometry' not in f:\n",
    "                print('No geometry key in entry {}'.format(f))\n",
    "            c = shp.shape(f['geometry']).centroid\n",
    "            p = (c.y, c.x)\n",
    "\n",
    "            d = round(distance(p, center).m * 10**6)/10**6        # round to micrometers\n",
    "            f['centroid'] = p            \n",
    "\n",
    "            if d in data_raw :\n",
    "                if d in data_duplicates :   \n",
    "                    data_duplicates[d].append(f)      # add to list in existing dictionary key\n",
    "                else :\n",
    "                    data_duplicates[d] = [data_raw[d], f]      # create list in existing dictionary key\n",
    "            else :\n",
    "                data_raw[d] = f\n",
    "print('Number of parcels in dict: %d\\n' % len(data_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are now about 97000 entries, about 8000 less than the original file.  It turns out that these are duplicate entries, such as condominiums, that share the same street address and coordinates but have different assessor parcel numbers (APNs).  Interestingly, the micrometer precision is necessary to distinguish parcels: rounding only to millimeters results in clashes between parcels that are distinct.  Given the number of parcels at a certain radius (when large), this isn't terribly surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate parcels: 7633\n"
     ]
    }
   ],
   "source": [
    "duplicateCount = 0\n",
    "for (key, value) in data_duplicates.items() :\n",
    "    duplicateCount += len(value)\n",
    "print('Total number of duplicate parcels: %d' % (duplicateCount-len(data_duplicates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 7633+97718 = 105351, this accounts for the entire difference.  Now, save this dictionary to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/OaklandParcels_inProcess.pkl', 'wb') as datafile :\n",
    "    a = pickle.Pickler(datafile)\n",
    "    compressed = {}\n",
    "    compressed['data_raw'] = data_raw\n",
    "    compressed['data_queried'] = {}\n",
    "    compressed['data_errors'] = []\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding year built from Zillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the processing to work piece-wise because of the 1000 queries/day limit, the input dictionary is processed starting with the smallest key.  After each entry is sent to the zillow API, it is popped from the input dictionary and placed in the output dictionary if the response is valid.  If the response is invalid, it is placed into an error dictionary for later processing.  There's also a rate limit of 10 queries/second, so a timer around the loop slows it down to that rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Zillow variables keys\n",
    "with open('../private/API_keys.pkl', 'rb') as datafile:\n",
    "    zid = pickle.load(datafile)             # API key\n",
    "zurl = 'http://www.zillow.com/webservice/GetDeepSearchResults.htm?'\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'\n",
    "radius = 1000            # only process parcels within this radius - used for debugging\n",
    "numToProcess = 1000      # zillow API limits to 1000 queries per day\n",
    "\n",
    "# load data structures\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "data_errors = compressed['data_errors']\n",
    "del compressed\n",
    "\n",
    "# sort keys by distance from closest to furthest\n",
    "sortedKeys = [k for k in sorted(data_raw) if k < radius]\n",
    "\n",
    "for (i, key) in zip(range(numToProcess), sortedKeys) :\n",
    "    startT = time.time()            # set up timer to keep requests under 10/s\n",
    "\n",
    "    try :\n",
    "        # read in address details from input dictionary\n",
    "        zp = {'address' : '{} {} {}'.format(data_raw[key]['properties']['ADDR_HN'],\n",
    "                      data_raw[key]['properties']['ADDR_SN'],\n",
    "                      data_raw[key]['properties']['ADDR_ST']),\n",
    "              'citystatezip' : 'Oakland, CA ' + str(data_raw[key]['properties']['ZIP']),\n",
    "              'zws-id' : zid}\n",
    "        r = requests.get(zurl, params=zp)\n",
    "        r_dict = xmltodict.parse(r.text)['SearchResults:searchresults']\n",
    "\n",
    "        if r_dict['message']['code'] == '0' :       # valid response?\n",
    "            r_dict = r_dict['response']['results']['result']\n",
    "            \n",
    "            # in case the response is a list of multiple (similar) entries, take the first one\n",
    "            if type(r_dict)==list :\n",
    "                r_dict = r_dict[0]\n",
    "            # prune extraneous fields\n",
    "            r_dict.pop('links')\n",
    "            r_dict.pop('zestimate')\n",
    "            r_dict.pop('localRealEstate')\n",
    "            data_raw[key]['zillow'] = r_dict\n",
    "            # transfer  to output dictionary\n",
    "            data_queried[key] = data_raw.pop(key)\n",
    "            \n",
    "        else :\n",
    "            print('For request {}, zillow code is {}. Here''s the record:'.format(\n",
    "                    zp['address'], r_dict['message']['code']))\n",
    "            print(r_dict)\n",
    "            print('-'*60)\n",
    "            # transfer info to error dictionary for offline analysis\n",
    "            data_errors.append({'key': key, 'value': data_raw.pop(key), \n",
    "                                  'zillow': r_dict, 'source': 'zillow'})\n",
    "    except Exception as exc:\n",
    "        print('Unspecified error: {}'.format(exc))\n",
    "        data_errors.append({'key': key, 'value': data_raw.pop(key),\n",
    "                              'source': 'exception'})\n",
    "            \n",
    "    # log status\n",
    "    print(i, ' ', zp['address'])\n",
    "\n",
    "    endT = time.time()\n",
    "    if endT - startT < 0.2 :\n",
    "        time.sleep(0.2 - (endT-startT))     # rate limit to 5 calls per second\n",
    "\n",
    "# save dictionaries back to disk\n",
    "compressed = {}\n",
    "compressed['data_raw'] = data_raw\n",
    "compressed['data_queried'] = data_queried\n",
    "compressed['data_errors'] = data_errors\n",
    "with open(inProcessFile, 'wb') as fid :\n",
    "    a = pickle.Pickler(fid)\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common error was code 508, 'no exact match found for input address'.  This was caused by the address not being in the Zillow database, such as the case for commercial buildings, churches, schools, etc., but could also be caused by an invalid address, such as a parcel without a street address ('None MacArthur Blvd').  The error rate was about 5%, or 1 in 20.  These results will be shown a little bit later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step to preparing the database is to write it back to a shapefile so it can be easily projected onto a choropleth map.  The schema from the original file is modified to include useful fields ('yearBuilt') and extra fields are discarded to speed up processing time.  Because this shapefile uses an ordered dictionary, the data fields need to be rearranged to match the schema's order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fiona\n",
    "import pickle\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'         # data source\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "del compressed\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'       # source of shape info in dictionary\n",
    "outputFileName = 'Oakland_parcels_queried'\n",
    "radius = 2000                                   # in m\n",
    "\n",
    "# create output directory if it doesn't exist yet\n",
    "if os.path.isdir('data/' + outputFileName) is False :\n",
    "    os.makedirs('data/' + outputFileName)\n",
    "outputFile = 'data/' + outputFileName + '/' + outputFileName + '.shp'\n",
    "\n",
    "# Register format drivers with a context manager\n",
    "with fiona.drivers():\n",
    "    # get schema from original file\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "        meta = source.meta\n",
    "        \n",
    "    # add new fields to schema file\n",
    "    meta['schema']['centroid'] = ('float:19:11', 'float:19:11')\n",
    "    meta['schema']['id'] = 'float:19'\n",
    "    meta['schema']['type'] = 'str:50'\n",
    "    meta['schema']['yearBuilt'] = 'float:10'\n",
    "    meta['schema']['properties']['YEARBUILT'] = 'int:6'\n",
    "    meta['schema']['properties'] = OrderedDict(meta['schema']['properties'])\n",
    "    schemaOrder = meta['schema']['properties']\n",
    "\n",
    "    with fiona.open(outputFile, 'w', **meta) as sink:\n",
    "        for (i,f) in enumerate(data_queried) :\n",
    "            if f <= radius :   \n",
    "                if 'yearBuilt' in data_queried[f]['zillow'] :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = data_queried[f]['zillow']['yearBuilt']\n",
    "                    data_queried[f].pop('zillow')\n",
    "                else :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = np.nan\n",
    "                # reorder dictionary to match schema order\n",
    "                data_queried[f]['properties'] = OrderedDict(\n",
    "                        (k, data_queried[f]['properties'][k]) for k in schemaOrder)           \n",
    "                sink.write(data_queried[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw a Choropleth map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to show the build year is graphically on a map, which will give an idea of which spatial patterns might require more analysis.  There is no straightforward implementation of this simple question, but there are multiple packages in python that can be strung together to draw a map, as well as a multitude of commercial software that does the same thing.\n",
    "\n",
    "The general overview is to first choose a projection grid to map the round world onto.  This converts the (longitude, latitude) pairs to (x, y) pairs for the projection.  On top of this, the parcel polygons are drawn and colored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "from descartes import PolygonPatch\n",
    "from itertools import chain\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "\n",
    "# shapefile database\n",
    "baseFile = 'data/Oakland_parcels_queried/Oakland_parcels_queried'\n",
    "\n",
    "center = geopy.Point(37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "radius = 0.25                           # in km\n",
    "ur = distance(kilometers=radius*2**0.5).destination(center, +45)\n",
    "ll = distance(kilometers=radius*2**0.5).destination(center, -135)\n",
    "ur = (ur.longitude, ur.latitude)\n",
    "ll = (ll.longitude, ll.latitude)\n",
    "extra = 0.01           # padding for edges\n",
    "coords = list(chain(ll, ur))\n",
    "w, h = coords[2] - coords[0], coords[3] - coords[1]\n",
    "\n",
    "m = Basemap(\n",
    "    projection='tmerc',\n",
    "    lon_0=-122.,\n",
    "    lat_0=37.,\n",
    "    ellps = 'WGS84',\n",
    "    llcrnrlon=coords[0] - extra * w,\n",
    "    llcrnrlat=coords[1] - extra * h,\n",
    "    urcrnrlon=coords[2] + extra * w,\n",
    "    urcrnrlat=coords[3] + extra * h,\n",
    "    lat_ts=0,\n",
    "    resolution='i',\n",
    "    suppress_ticks=True)\n",
    "\n",
    "m.readshapefile(\n",
    "    baseFile,\n",
    "    'oakland',\n",
    "    color='blue',\n",
    "    zorder=2)\n",
    "  \n",
    "# set up a map dataframe\n",
    "df_map = pd.DataFrame({\n",
    "    'poly': [Polygon(xy) for xy in m.oakland],\n",
    "    'id': [obj['OBJECTID'] for obj in m.oakland_info],\n",
    "    'zip': [obj['ZIP'] for obj in m.oakland_info],\n",
    "    'yearBuilt': [obj['YEARBUILT'] for obj in m.oakland_info]})\n",
    "\n",
    "# Create projection view as a polygon to filter shapes\n",
    "window = [ll, (ll[0], ur[1]), ur, (ur[0], ll[1]), ll]\n",
    "window = list(zip( *m(*list(zip(*window))) ))\n",
    "window_map = pd.DataFrame({'poly': [Polygon(window)]})\n",
    "window_polygon = prep(MultiPolygon(list(window_map['poly'].values)))\n",
    "\n",
    "# Remove any shapes that are outside the map window\n",
    "df_map = df_map[ [window_polygon.intersects(i) for i in df_map.poly] ]\n",
    "\n",
    "# draw tract patches from polygons\n",
    "df_map['patches'] = df_map['poly'].map(lambda x: PolygonPatch(\n",
    "    x,\n",
    "    ec='#787878', lw=.25, alpha=.9,\n",
    "    zorder=4))\n",
    "\n",
    "# create colormap based on year built\n",
    "cmap_range = (1880, 1940)\n",
    "ncolors = 8\n",
    "yearBuilt_bins = np.linspace(min(cmap_range), max(cmap_range), ncolors+1)\n",
    "cmap = matplotlib.cm.coolwarm\n",
    "cmap.set_bad(color='white')       # if yearBuilt is nan\n",
    "norm = matplotlib.colors.BoundaryNorm(yearBuilt_bins, ncolors)\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
    "\n",
    "# plot parcels by adding the PatchCollection to the axes instance\n",
    "pc = PatchCollection(df_map['patches'].values, match_original=True)\n",
    "pc.set_facecolor(cmap(norm(df_map.yearBuilt)/ncolors));\n",
    "ax.add_collection(pc)\n",
    "\n",
    "yearBuilt_labels = ['%.0f-%.0f' % (yearBuilt_bins[i], yearBuilt_bins[i+1])\n",
    "                        for i in range(ncolors)]\n",
    "yearBuilt_labels.append('>%.0f' % yearBuilt_bins[-1])\n",
    "\n",
    "cb = colorbar_index(ncolors=ncolors+1, cmap=cmap, shrink=0.5, labels=yearBuilt_labels)\n",
    "cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "# Draw a map scale\n",
    "m.drawmapscale(\n",
    "    coords[0] + w * 0.5, coords[1] + h * 0.1,\n",
    "    coords[0], coords[1],\n",
    "    radius/2*1000,   # length\n",
    "    barstyle='fancy', labelstyle='simple',\n",
    "    units = 'm',\n",
    "#    format='%.2f',\n",
    "    fillcolor1='w', fillcolor2='#555555',\n",
    "    fontcolor='#555555',\n",
    "    zorder=4)\n",
    "plt.title(\"Parcels in Oakland, labeled by year built\")\n",
    "plt.tight_layout()\n",
    "# this will set the image width to 722px at 100dpi\n",
    "fig.set_size_inches(7.22, 5.25)  # use for larger size\n",
    "#fig.set_size_inches(3.5, 2.5)\n",
    "plt.savefig('data/Oakland_temp.png', dpi=300, alpha=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data from Zillow is not completely correct.  For instance, 2901 Park Blvd (corner of Park and McKinley) was built around 1912 (according to https://localwiki.org/oakland/Mary_Smith_Home_for_Friendless_Girls), while Zillow puts it at 1930.\n",
    "\n",
    "Other old houses cited in this neighborhood include 552 Montclair (built in 1897 according to Zillow, not contradictory accounts), and 1047 Bella Vista was apparently built in 1892 (https://oaklandwiki.org/Fenton_Home_Orphanage), 18 years before Zillow claims it.  (An interesting aside: Susan Fenton, the sister of the woman who founded Fenton's Creamery, which is still operating on Piedmont St and as beloved as ever, founded a Home for Destitue Children here in 1925.)\n",
    "\n",
    "The Kaiser house, where Henry Kaiser lived between 1925 and the mid 1940s, was built in 1924 (Zillow: 1925).  \n",
    "\n",
    "More historical citations can be found in \"An Architectural Guidebook to San Francisco and the Bay Area\", by Susan Dinkelspiel Cerny (2007), online at https://books.google.com/books?id=FkVQx6MWa8MC&lpg=RA2-PA120&ots=OANNWFMFSG&dq=%221047%20bella%20vista%22%20oakland&pg=RA2-PA120#v=onepage&q=%221047%20bella%20vista%22%20oakland&f=false\n",
    "\n",
    "Based on this comparison, it seems like houses older than around 1920 are less likely to be accurately labeled in Zillow's archives than more recent houses.  A more complete analysis of this would require substantial research.  Here, I'm more interested in broad trends.\n",
    "\n",
    "Arbor Villa, Francis \"Borax\" Smith's estate just south of Park Blvd, was built between 1893-1895, and was torn down around 1932 after its grounds had already been subdivided and built upon.  Moved into house at 817 E 24th St., but Zillow says current house was built in 1912, despite this blog post (https://localwiki.org/oakland/Borax_Smith’s_Red_House)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
