{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development history of a neighborhood in Oakland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was curious how my neighborhood in Oakland, Cleveland Heights, was developed: which house was built first?  Which last?  How did this pattern fit with how the city was developed.  In particular, how much of that history can be read now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find that data for this, I needed to know when each house was built, where each house was, and a list of houses.  Thanks in large part to the great collection of Oakland data at XXURLXX.  Initially, I found a GeoJSON file for containing parcel info for the county of Alameda, which includes Oakland and extends from Berkeley to Fremont north-to-south, and from the Bay to beyond Livermore east-to-west.  This was a huge file, and couldn't be processed in memory.  Several days later, I found parcel info for the City of Oakland from 20XX that conveniently fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels 105351\n",
      "\n",
      "Second parcel:\n",
      "{'geometry': {'coordinates': [[(-122.244024425766, 37.86867162821933),\n",
      "                               (-122.24398667467038, 37.86866162294282),\n",
      "                               (-122.24399852865695, 37.868349112878676),\n",
      "                               (-122.24401355395213, 37.86835277617628),\n",
      "                               (-122.24404457097216, 37.868359870908144),\n",
      "                               (-122.2440757393451, 37.86836653553029),\n",
      "                               (-122.24408444864746, 37.86836826921976),\n",
      "                               (-122.24407290434733, 37.86867278199862),\n",
      "                               (-122.244024425766, 37.86867162821933)]],\n",
      "              'type': 'Polygon'},\n",
      " 'id': '1',\n",
      " 'properties': OrderedDict([('OBJECTID', 23),\n",
      "                            ('ADDR_HN', None),\n",
      "                            ('ADDR_PD', None),\n",
      "                            ('ADDR_SN', 'DWIGHT'),\n",
      "                            ('ADDR_ST', 'WAY'),\n",
      "                            ('OBJECTID_1', 116),\n",
      "                            ('APN', '048H770309700'),\n",
      "                            ('ADDRNUM', 0),\n",
      "                            ('STNAME', 'DWIGHT WY'),\n",
      "                            ('UNIT', None),\n",
      "                            ('ZIP', 94704),\n",
      "                            ('DATASRC', 'ASSR_20031023'),\n",
      "                            ('ORIGKEY', '048H770309700'),\n",
      "                            ('ADDRESS_AP', '048H770309700'),\n",
      "                            ('APNSRC', None),\n",
      "                            ('ADDRNUM_TX', None),\n",
      "                            ('SHAPE_AREA', 2830.19306092),\n",
      "                            ('SHAPE_LEN', 276.033464863)]),\n",
      " 'type': 'Feature'}\n"
     ]
    }
   ],
   "source": [
    "# insert code for reading shape file, print an entry\n",
    "import fiona\n",
    "import pprint\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "source = fiona.open(baseFile + '.shp')\n",
    "print('Number of parcels: %d\\n' % len(source))\n",
    "print('Second parcel:')\n",
    "source.next()    # first entry has a lot of coordinates, so skip\n",
    "pprint.pprint(source.next())\n",
    "\n",
    "source.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, each entry also includes fields for the street address, though this information is missing here.  I'll catch these errors below.  If the street address wasn't present, it another call to reverse geocode the location (from Google, for instance) would have been required.  Note that there are 105,000 entries in all.\n",
    "\n",
    "The construction history exists on Zillow, so this database was completed by repeat API calls.  The free Zillow API key that I have only allows 1000 queries per day.  Because I'm mostly interested in one neighborhood, I can approach this rate limit in a smart way by working outward from a central point.  I'll chooose the center location to be the Armenian Church on the top of the hill at McKinley and Spruce, since it's a big landmark close that's pretty central anyways.  (Another choice would have been the elmenetary school 3 blocks away.)  It will take 105 days (~3.5 months) to process all the data for Oakland.\n",
    "\n",
    "To do this radial processing, the shapefile array was rewritten as a dictionary with the key being the distance from the landmark.  The distance was computed from the centroid of the parcel shape.  Technically, computing the centroid is unnecessary for radial proessing - the first coordinate of the parcel shape would suffice - but this may be useful later on.  Also, it doesn't take much time on such a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels in dict: 97718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "import shapely.geometry as shp\n",
    "import pickle \n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "center = (37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "\n",
    "data_raw = {}\n",
    "data_duplicates = {}\n",
    "\n",
    "with fiona.drivers():           # Register format drivers with a context manager\n",
    "\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "       \n",
    "        for f in source :\n",
    "            if 'geometry' not in f:\n",
    "                print('No geometry key in entry {}'.format(f))\n",
    "            c = shp.shape(f['geometry']).centroid\n",
    "            p = (c.y, c.x)\n",
    "\n",
    "            d = round(distance(p, center).m * 10**6)/10**6        # round to micrometers\n",
    "            f['centroid'] = p            \n",
    "\n",
    "            if d in data_raw :\n",
    "                if d in data_duplicates :   \n",
    "                    data_duplicates[d].append(f)      # add to list in existing dictionary key\n",
    "                else :\n",
    "                    data_duplicates[d] = [data_raw[d], f]      # create list in existing dictionary key\n",
    "            else :\n",
    "                data_raw[d] = f\n",
    "print('Number of parcels in dict: %d\\n' % len(data_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are now about 97000 entries, about 8000 less than the original file.  It turns out that these are duplicate entries, such as condominiums, that share the same street address and coordinates but have different assessor parcel numbers (APNs).  Interestingly, the micrometer precision is necessary to distinguish parcels: rounding only to millimeters results in clashes between parcels that are distinct.  Given the number of parcels at a certain radius (when large), this isn't terribly surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate parcels: 7633\n"
     ]
    }
   ],
   "source": [
    "duplicateCount = 0\n",
    "for (key, value) in data_duplicates.items() :\n",
    "    duplicateCount += len(value)\n",
    "print('Total number of duplicate parcels: %d' % (duplicateCount-len(data_duplicates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 7633+97718 = 105351, this accounts for the entire difference.  Now, save this dictionary to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/OaklandParcels_inProcess.pkl', 'wb') as datafile :\n",
    "    a = pickle.Pickler(datafile)\n",
    "    compressed = {}\n",
    "    compressed['data_raw'] = data_raw\n",
    "    compressed['data_queried'] = {}\n",
    "    compressed['data_errors'] = []\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding year built from Zillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the processing to work piece-wise because of the 1000 queries/day limit, the input dictionary is processed starting with the smallest key.  After each entry is sent to the zillow API, it is popped from the input dictionary and placed in the output dictionary if the response is valid.  If the response is invalid, it is placed into an error dictionary for later processing.  There's also a rate limit of 10 queries/second, so a timer around the loop slows it down to that rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Zillow variables keys\n",
    "with open('../private/API_keys.pkl', 'rb') as datafile:\n",
    "    zid = pickle.load(datafile)             # API key\n",
    "zurl = 'http://www.zillow.com/webservice/GetDeepSearchResults.htm?'\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'\n",
    "radius = 1000            # only process parcels within this radius - used for debugging\n",
    "numToProcess = 1000      # zillow API limits to 1000 queries per day\n",
    "\n",
    "# load data structures\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "data_errors = compressed['data_errors']\n",
    "del compressed\n",
    "\n",
    "# sort keys by distance from closest to furthest\n",
    "sortedKeys = [k for k in sorted(data_raw) if k < radius]\n",
    "\n",
    "for (i, key) in zip(range(numToProcess), sortedKeys) :\n",
    "    startT = time.time()            # set up timer to keep requests under 10/s\n",
    "\n",
    "    try :\n",
    "        # read in address details from input dictionary\n",
    "        zp = {'address' : '{} {} {}'.format(data_raw[key]['properties']['ADDR_HN'],\n",
    "                      data_raw[key]['properties']['ADDR_SN'],\n",
    "                      data_raw[key]['properties']['ADDR_ST']),\n",
    "              'citystatezip' : 'Oakland, CA ' + str(data_raw[key]['properties']['ZIP']),\n",
    "              'zws-id' : zid}\n",
    "        r = requests.get(zurl, params=zp)\n",
    "        r_dict = xmltodict.parse(r.text)['SearchResults:searchresults']\n",
    "\n",
    "        if r_dict['message']['code'] == '0' :       # valid response?\n",
    "            r_dict = r_dict['response']['results']['result']\n",
    "            \n",
    "            # in case the response is a list of multiple (similar) entries, take the first one\n",
    "            if type(r_dict)==list :\n",
    "                r_dict = r_dict[0]\n",
    "            # prune extraneous fields\n",
    "            r_dict.pop('links')\n",
    "            r_dict.pop('zestimate')\n",
    "            r_dict.pop('localRealEstate')\n",
    "            data_raw[key]['zillow'] = r_dict\n",
    "            # transfer  to output dictionary\n",
    "            data_queried[key] = data_raw.pop(key)\n",
    "            \n",
    "        else :\n",
    "            print('For request {}, zillow code is {}. Here''s the record:'.format(\n",
    "                    zp['address'], r_dict['message']['code']))\n",
    "            print(r_dict)\n",
    "            print('-'*60)\n",
    "            # transfer info to error dictionary for offline analysis\n",
    "            data_errors.append({'key': key, 'value': data_raw.pop(key), \n",
    "                                  'zillow': r_dict, 'source': 'zillow'})\n",
    "    except Exception as exc:\n",
    "        print('Unspecified error: {}'.format(exc))\n",
    "        data_errors.append({'key': key, 'value': data_raw.pop(key),\n",
    "                              'source': 'exception'})\n",
    "            \n",
    "    # log status\n",
    "    print(i, ' ', zp['address'])\n",
    "\n",
    "    endT = time.time()\n",
    "    if endT - startT < 0.2 :\n",
    "        time.sleep(0.2 - (endT-startT))     # rate limit to 5 calls per second\n",
    "\n",
    "# save dictionaries back to disk\n",
    "compressed = {}\n",
    "compressed['data_raw'] = data_raw\n",
    "compressed['data_queried'] = data_queried\n",
    "compressed['data_errors'] = data_errors\n",
    "with open(inProcessFile, 'wb') as fid :\n",
    "    a = pickle.Pickler(fid)\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common error was code 508, 'no exact match found for input address'.  This was caused by the address not being in the Zillow database, such as the case for commercial buildings, churches, schools, etc., but could also be caused by an invalid address, such as a parcel without a street address ('None MacArthur Blvd').  The error rate was about 5%, or 1 in 20.  These results will be shown a little bit later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step to preparing the database is to write it back to a shapefile so it can be easily projected onto a choropleth map.  The schema from the original file is modified to include useful fields ('yearBuilt') and extra fields are discarded to speed up processing time.  Because this shapefile uses an ordered dictionary, the data fields need to be rearranged to match the schema's order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fiona\n",
    "import pickle\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'         # data source\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "del compressed\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'       # source of shape info in dictionary\n",
    "outputFileName = 'Oakland_parcels_queried'\n",
    "radius = 2000                                   # in m\n",
    "\n",
    "# create output directory if it doesn't exist yet\n",
    "if os.path.isdir('data/' + outputFileName) is False :\n",
    "    os.makedirs('data/' + outputFileName)\n",
    "outputFile = 'data/' + outputFileName + '/' + outputFileName + '.shp'\n",
    "\n",
    "# Register format drivers with a context manager\n",
    "with fiona.drivers():\n",
    "    # get schema from original file\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "        meta = source.meta\n",
    "        \n",
    "    # add new fields to schema file\n",
    "    meta['schema']['centroid'] = ('float:19:11', 'float:19:11')\n",
    "    meta['schema']['id'] = 'float:19'\n",
    "    meta['schema']['type'] = 'str:50'\n",
    "    meta['schema']['yearBuilt'] = 'float:10'\n",
    "    meta['schema']['properties']['YEARBUILT'] = 'int:6'\n",
    "    meta['schema']['properties'] = OrderedDict(meta['schema']['properties'])\n",
    "    schemaOrder = meta['schema']['properties']\n",
    "\n",
    "    with fiona.open(outputFile, 'w', **meta) as sink:\n",
    "        for (i,f) in enumerate(data_queried) :\n",
    "            if f <= radius :   \n",
    "                if 'yearBuilt' in data_queried[f]['zillow'] :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = data_queried[f]['zillow']['yearBuilt']\n",
    "                    data_queried[f].pop('zillow')\n",
    "                else :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = np.nan\n",
    "                # reorder dictionary to match schema order\n",
    "                data_queried[f]['properties'] = OrderedDict(\n",
    "                        (k, data_queried[f]['properties'][k]) for k in schemaOrder)           \n",
    "                sink.write(data_queried[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw a Choropleth map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to show the build year is graphically on a map, which will give an idea of which spatial patterns might require more analysis.  There is no straightforward implementation of this simple question, but there are multiple packages in python that can be strung together to draw a map, as well as a multitude of commercial software that does the same thing.\n",
    "\n",
    "The general overview is to first choose a projection grid to map the round world onto.  This converts the (longitude, latitude) pairs to (x, y) pairs for the projection.  On top of this, the parcel polygons are drawn and colore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
