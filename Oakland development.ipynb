{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development history of a neighborhood in Oakland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growing up in a small town on the East Coast, I knew the person who builtour house.  Not personally, mind you, but I knew that he was the brother of the father of our next-door-neighbors (who were themselves quite old when I was a child).  Then I began to move around: first for college, then for graduate school, then to the Bay Area.  When I recently moved to Oakland and started putting down roots, I was missing the same kind of connection to my new neighborhood.  So I got a copy of Beth Bagwell's book [*Oakland The Story of a City*](http://blog.ouroakland.net/2012/05/oakland-story-of-city.html), which filled in some broad background: first the Muwekma Ohlone villages and shell mounds, then the Spanish land grants to the rancheros (Peralta and his sons), then the gold rush and the settlers who stole the land, the development of downtown, and then the gradual spreading of the city boundary with mass transit.  Then my neighborhood was built.  This was followed by WWII and the influx of African-Americans to work in the war effort, the grimmer recent history of the city center being abandoned for the suburbs, and the recent tech boom and its influx of wealth.\n",
    "\n",
    "But somewhere in that narrative was my question, still unanswered: how was my neighborhood constructed?  Sure, one can find old maps that list the development parcels, but what did the neighborhood look like going up?  Was it built all at once in the roaring 20s, or piece-meal, one-farmhouse here, another there, as the land slowly became subdivided and the orchards and oak trees cut down?  This is a narrow question, and more recent history is layered on top of this: how the highways divided neighbors, how new apartment buildings went up, which structures burned down or were destroyed.  And it also ignores how the land was used by the Peraltas for decades, and the Ohlone for many centuries before them.  But the further back one looks, the fainter the traces.  Or, said another way: you have to start somewhere.\n",
    "\n",
    "So I decided to answer the simplest question I could: when was each house built?  This required some information that shouldn't be too hard to find: a list of houses, where each one is, and when it was built.  I'll describe the process of analyzing this data as I go along, complete with code snippets in case others want to extend it.  Standalone scripts can be found in this repository as well, as indicated in comments.\n",
    "\n",
    "To skip right to the analysis, scroll down to the maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data sets made available as part of the [CodeForOakland](http://codeforoakland.org/data-sets/#oakland1) project have been crucial.  Initially, I found a GeoJSON file for containing parcel info for the county of Alameda, which includes Oakland and extends north-to-south from Berkeley to Fremont, and east-to-west from the Bay to beyond Livermore.  This was a huge file with 10x the data of just Oakland, couldn't be processed in memory, and lacked street addresses.  Several days later, I found a shapefile of all parcels in Oakland, from around 2011, that conveniently fit into memory.  Still, if I want to go back and look at the info for Piedmont (or neighboring cities), the data's there.\n",
    "\n",
    "This is the info for each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels 105351\n",
      "\n",
      "Second parcel:\n",
      "{'geometry': {'coordinates': [[(-122.244024425766, 37.86867162821933),\n",
      "                               (-122.24398667467038, 37.86866162294282),\n",
      "                               (-122.24399852865695, 37.868349112878676),\n",
      "                               (-122.24401355395213, 37.86835277617628),\n",
      "                               (-122.24404457097216, 37.868359870908144),\n",
      "                               (-122.2440757393451, 37.86836653553029),\n",
      "                               (-122.24408444864746, 37.86836826921976),\n",
      "                               (-122.24407290434733, 37.86867278199862),\n",
      "                               (-122.244024425766, 37.86867162821933)]],\n",
      "              'type': 'Polygon'},\n",
      " 'id': '1',\n",
      " 'properties': OrderedDict([('OBJECTID', 23),\n",
      "                            ('ADDR_HN', None),\n",
      "                            ('ADDR_PD', None),\n",
      "                            ('ADDR_SN', 'DWIGHT'),\n",
      "                            ('ADDR_ST', 'WAY'),\n",
      "                            ('OBJECTID_1', 116),\n",
      "                            ('APN', '048H770309700'),\n",
      "                            ('ADDRNUM', 0),\n",
      "                            ('STNAME', 'DWIGHT WY'),\n",
      "                            ('UNIT', None),\n",
      "                            ('ZIP', 94704),\n",
      "                            ('DATASRC', 'ASSR_20031023'),\n",
      "                            ('ORIGKEY', '048H770309700'),\n",
      "                            ('ADDRESS_AP', '048H770309700'),\n",
      "                            ('APNSRC', None),\n",
      "                            ('ADDRNUM_TX', None),\n",
      "                            ('SHAPE_AREA', 2830.19306092),\n",
      "                            ('SHAPE_LEN', 276.033464863)]),\n",
      " 'type': 'Feature'}\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import pprint\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "source = fiona.open(baseFile + '.shp')\n",
    "print('Number of parcels: %d\\n' % len(source))\n",
    "print('Second parcel:')\n",
    "source.next()    # first entry has a lot of coordinates, so skip\n",
    "pprint.pprint(source.next())\n",
    "\n",
    "source.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this database has a field for the street address, though the specific example above lacks this information ('None').  I'll catch this kind of error below.  If the street address wasn't present, it could be found by reverse geocoding the location (from Google's API, for instance).  All told, there are 105,000 entries.\n",
    "\n",
    "The construction history exists in Zillow's database, so the dataset was completed by calling Zillow's API for each property.  The one catch is that Zillow's free API key allows only 1000 queries per day.  Because I'm mostly interested in a few neighborhoods, I can approach this rate limit in a smart way by working outward from a central point; radial processing, if you will.  I'll chooose the center location to be a landmark in my neighborhood of Cleveland Heights: the Armenian Church on the top of the hill, at the corner of McKinley and Spruce.  At this rate, it will take 105 days (~3.5 months) to process all the data for Oakland; but all the surrounding neighborhoods should take a couple of weeks at most.\n",
    "\n",
    "To process the parcels radially, the shapefile array was rewritten as a dictionary with the key being the distance between the landmark and the centroid of each parcel.  Notice the data stays as a dictionary to facilitate saving it back to a shape file at the end, which is useful for mapping purposes.  If that wasn't a concern, it would make a lot of sense to convert the format to something easier to manipulate, like a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels in dict: 97718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "import shapely.geometry as shp\n",
    "import pickle \n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "center = (37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "\n",
    "data_raw = {}\n",
    "data_duplicates = {}\n",
    "\n",
    "with fiona.drivers():           # Register format drivers with a context manager\n",
    "\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "       \n",
    "        for f in source :\n",
    "            if 'geometry' not in f:\n",
    "                print('No geometry key in entry {}'.format(f))\n",
    "            c = shp.shape(f['geometry']).centroid\n",
    "            p = (c.y, c.x)\n",
    "\n",
    "            d = round(distance(p, center).m * 10**6)/10**6        # round to micrometers\n",
    "            f['centroid'] = p            \n",
    "\n",
    "            if d in data_raw :\n",
    "                if d in data_duplicates :   \n",
    "                    data_duplicates[d].append(f)      # add to list in existing dictionary key\n",
    "                else :\n",
    "                    data_duplicates[d] = [data_raw[d], f]      # create list in existing dictionary key\n",
    "            else :\n",
    "                data_raw[d] = f\n",
    "print('Number of parcels in dict: %d\\n' % len(data_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, computing the centroid is unnecessary for radial processing - the first coordinate of the parcel shape would suffice - but this info may be useful later on.  Also, the amount of time it adds to processing is negligible for such a small dataset.\n",
    "\n",
    "Notice that there are now about 97000 entries, about 8000 less than the original file.  It turns out that these are duplicate entries, such as condominiums, that share the same street address and coordinates but have different assessor parcel numbers (APNs).  Doing the math confirms that this accounts for all duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate parcels: 7633\n"
     ]
    }
   ],
   "source": [
    "duplicateCount = 0\n",
    "for (key, value) in data_duplicates.items() :\n",
    "    duplicateCount += len(value)\n",
    "print('Total number of duplicate parcels: %d' % (duplicateCount-len(data_duplicates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$7633+97718 = 105351$, which was the original number of entries in the shapefile.\n",
    "\n",
    "Interestingly, the micrometer precision in the distance key is necessary to distinguish parcels: rounding to millimeters results in clashes between different street addresses.  Given the number of parcels at a certain radius once the radius gets large, this isn't terribly surprising, but still an nice example about the importance of precision and probability.\n",
    "\n",
    "Now the cleaned dataset is saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/OaklandParcels_inProcess.pkl', 'wb') as datafile :\n",
    "    a = pickle.Pickler(datafile)\n",
    "    compressed = {}\n",
    "    compressed['data_raw'] = data_raw\n",
    "    compressed['data_queried'] = {}\n",
    "    compressed['data_errors'] = []\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is processed starting with the smallest key and working outwards: each entry is first sent to the zillow API, then popped from the input dictionary (data_raw) and placed in the output dictionary (data_queried) if the response is valid.  If the response is invalid, it is placed into an error dictionary (data_errors) for later processing.  There's also a rate limit of 10 queries/second, so a timer around the loop limits the rate.  It runs at 5 queries/second just to be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CreateParcelDatabase.py\n",
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Zillow variables keys\n",
    "with open('../private/API_keys.pkl', 'rb') as datafile:\n",
    "    zid = pickle.load(datafile)             # API key\n",
    "zurl = 'http://www.zillow.com/webservice/GetDeepSearchResults.htm?'\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'\n",
    "radius = 50000            # only process parcels within this radius - set lower for debugging\n",
    "numToProcess = 1000      # zillow API limits to 1000 queries per day\n",
    "\n",
    "# load data structures\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "data_errors = compressed['data_errors']\n",
    "del compressed\n",
    "\n",
    "# sort keys by distance from closest to furthest\n",
    "sortedKeys = [k for k in sorted(data_raw) if k < radius]\n",
    "\n",
    "numProcessed = 0\n",
    "while numProcessed < numToProcess and len(sortedKeys) > 0 : \n",
    "    key = sortedKeys.pop(0)\n",
    "    startT = time.time()    # set up timer to keep requests under 10/s\n",
    "\n",
    "    zp = {'address' : '{} {} {}'.format(data_raw[key]['properties']['ADDR_HN'],\n",
    "                  data_raw[key]['properties']['ADDR_SN'],\n",
    "                  data_raw[key]['properties']['ADDR_ST']),\n",
    "          'citystatezip' : 'Oakland, CA ' + str(data_raw[key]['properties']['ZIP']),\n",
    "          'zws-id' : zid}\n",
    "    if ( data_raw[key]['properties']['ADDR_HN'] and \n",
    "        data_raw[key]['properties']['ADDR_SN'] and\n",
    "        data_raw[key]['properties']['ADDR_ST'] ) :\n",
    "        try :         # read in address details from input dictionary\n",
    "            r = requests.get(zurl, params=zp)\n",
    "            r_dict = xmltodict.parse(r.text)['SearchResults:searchresults']\n",
    "    \n",
    "            if r_dict['message']['code'] == '0' :       # valid response?\n",
    "                r_dict = r_dict['response']['results']['result']\n",
    "                \n",
    "                # in case the response is a list of multiple entries, take only the first one\n",
    "                if type(r_dict)==list :\n",
    "                    r_dict = r_dict[0]\n",
    "                # prune result dictionary\n",
    "                r_dict.pop('links')\n",
    "                r_dict.pop('zestimate')\n",
    "                r_dict.pop('localRealEstate')\n",
    "                data_raw[key]['zillow'] = r_dict\n",
    "                # transfer  to output dictionary\n",
    "                data_queried[key] = data_raw.pop(key)\n",
    "                \n",
    "            else :\n",
    "                print('For request {}, zillow code is {}. Here''s the record:'.format(zp['address'], \n",
    "                                                                                      r_dict['message']['code']))\n",
    "                print(r_dict)\n",
    "                print('-'*60)\n",
    "                # transfer info to error dictionary for offline analysis\n",
    "                data_errors.append({'key': key, 'value': data_raw.pop(key), \n",
    "                                      'zillow': r_dict, 'source': 'zillow'})\n",
    "        except Exception as exc:\n",
    "            print('Unspecified error: {}'.format(exc))\n",
    "            data_errors.append({'key': key, 'value': data_raw.pop(key),\n",
    "                                  'source': 'exception'})\n",
    "        numProcessed += 1\n",
    "        print(numProcessed, ' ', zp['address'])                   # log status to command line\n",
    "    else :      # invalid address\n",
    "        print('\\tInvalid address: ' + zp['address'])\n",
    "        data_errors.append({'key': key, 'value': data_raw.pop(key),\n",
    "                                  'source': 'Invalid address'})\n",
    "   \n",
    "    endT = time.time()\n",
    "    if endT - startT < 0.2 :\n",
    "        time.sleep(0.2 - (endT-startT))     # rate limit to 5 calls per second, just to be nice\n",
    "\n",
    "# save dictionaries back to disk\n",
    "compressed = {}\n",
    "compressed['data_raw'] = data_raw\n",
    "compressed['data_queried'] = data_queried\n",
    "compressed['data_errors'] = data_errors\n",
    "with open(inProcessFile, 'wb') as fid :\n",
    "    a = pickle.Pickler(fid)\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common error was code 508, 'no exact match found for input address'.  This was primarily caused by the address not being in the Zillow database, such as for commercial buildings, churches, schools, etc.  But this was also caused by invalid address, such as a parcel without a street address ('None MacArthur Blvd'), which was the case for parks, municipal land, and Lake Merrit.  The error rate was about 5%, or 1 in 20.\n",
    "\n",
    "Before mapping the data, it needs to be written back to a shapefile so it can be easily processed.  Even though shapefiles are an old format, they are pretty efficient for this kind of processing - much more so than the GeoJSON format.  When doing so, the schema from the original file needs to be modified to include the new data ('yearBuilt').  Extra fields from zillow are jettisoned to speed up processing.  Finally, because this shapefile has the 'properties' dictionary in an OrderedDict class, the data fields need to be rearranged to match the schema's order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SaveParcelDictionaryAsShapefile.py\n",
    "import fiona\n",
    "import pickle\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'         # data source\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "del compressed\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'       # source of shape info in dictionary\n",
    "outputFileName = 'Oakland_parcels_queried'\n",
    "radius = 2000                                   # in m\n",
    "\n",
    "# create output directory if it doesn't exist yet\n",
    "if os.path.isdir('data/' + outputFileName) is False :\n",
    "    os.makedirs('data/' + outputFileName)\n",
    "outputFile = 'data/' + outputFileName + '/' + outputFileName + '.shp'\n",
    "\n",
    "# Register format drivers with a context manager\n",
    "with fiona.drivers():\n",
    "    # get schema from original file\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "        meta = source.meta\n",
    "        \n",
    "    # add new fields to schema file\n",
    "    meta['schema']['centroid'] = ('float:19:11', 'float:19:11')\n",
    "    meta['schema']['id'] = 'float:19'\n",
    "    meta['schema']['type'] = 'str:50'\n",
    "    meta['schema']['yearBuilt'] = 'float:10'\n",
    "    meta['schema']['properties']['YEARBUILT'] = 'int:6'\n",
    "    meta['schema']['properties'] = OrderedDict(meta['schema']['properties'])\n",
    "    schemaOrder = meta['schema']['properties']\n",
    "\n",
    "    with fiona.open(outputFile, 'w', **meta) as sink:\n",
    "        for (i,f) in enumerate(data_queried) :\n",
    "            if f <= radius :   \n",
    "                if 'yearBuilt' in data_queried[f]['zillow'] :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = data_queried[f]['zillow']['yearBuilt']\n",
    "                    data_queried[f].pop('zillow')\n",
    "                else :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = np.nan\n",
    "                # reorder dictionary to match schema order\n",
    "                data_queried[f]['properties'] = OrderedDict(\n",
    "                        (k, data_queried[f]['properties'][k]) for k in schemaOrder)           \n",
    "                sink.write(data_queried[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kind of map used to show this data is a choropleth map, which maps a physical quantity (year built) onto a spatial extent by using some kind of shading.  This is a quick way to show what spatial patterns might require more analysis.  While Python has no straightforward way to do this, all the tools required are free and there's extensive support and code examples online.\n",
    "\n",
    "The general overview is to first choose a projection grid to map the round world onto.  This converts the (longitude, latitude) pairs to (x, y) pairs in the coordinate system of the projection.  The parcel polygons are drawn and colored on top of this.  First, though, here are two functions to make colorbars, borrowed from [Sensitive Cities](http://sensitivecities.com/so-youd-like-to-make-a-map-using-python-EN.html#.V2hnJa4tVVz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convenience functions for working with colour ramps and bars\n",
    "def colorbar_index(ncolors, cmap, labels=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This is a convenience function to stop you making off-by-one errors\n",
    "    Takes a standard colour ramp, and discretizes it,\n",
    "    then draws a colour bar with correctly aligned labels\n",
    "    \"\"\"\n",
    "    cmap = cmap_discretize(cmap, ncolors)\n",
    "    mappable = cm.ScalarMappable(cmap=cmap)\n",
    "    mappable.set_array([])\n",
    "    mappable.set_clim(-0.5, ncolors+0.5)\n",
    "    colorbar = plt.colorbar(mappable, **kwargs)\n",
    "    colorbar.set_ticks(np.linspace(0, ncolors, ncolors))\n",
    "    colorbar.set_ticklabels(range(ncolors))\n",
    "    if labels:\n",
    "        colorbar.set_ticklabels(labels)\n",
    "    return colorbar\n",
    "\n",
    "def cmap_discretize(cmap, N):\n",
    "    \"\"\"\n",
    "    Return a discrete colormap from the continuous colormap cmap.\n",
    "\n",
    "        cmap: colormap instance, eg. cm.jet. \n",
    "        N: number of colors.\n",
    "\n",
    "    Example\n",
    "        x = resize(arange(100), (5,100))\n",
    "        djet = cmap_discretize(cm.jet, 5)\n",
    "        imshow(x, cmap=djet)\n",
    "\n",
    "    \"\"\"\n",
    "    if type(cmap) == str:\n",
    "        cmap = get_cmap(cmap)\n",
    "    colors_i = np.concatenate((np.linspace(0, 1., N), (0., 0., 0., 0.)))\n",
    "    colors_rgba = cmap(colors_i)\n",
    "    indices = np.linspace(0, 1., N + 1)\n",
    "    cdict = {}\n",
    "    for ki, key in enumerate(('red', 'green', 'blue')):\n",
    "        cdict[key] = [(indices[i], colors_rgba[i - 1, ki], colors_rgba[i, ki]) for i in xrange(N + 1)]\n",
    "    return matplotlib.colors.LinearSegmentedColormap(cmap.name + \"_%d\" % N, cdict, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the mapping routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DrawParcelChoropleth.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "from descartes import PolygonPatch\n",
    "from itertools import chain\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "\n",
    "# shapefile database\n",
    "baseFile = 'data/Oakland_parcels_queried/Oakland_parcels_queried'\n",
    "\n",
    "center = geopy.Point(37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "radius = 0.6                           # in km\n",
    "ur = distance(kilometers=radius*2**0.5).destination(center, +45)\n",
    "ll = distance(kilometers=radius*2**0.5).destination(center, -135)\n",
    "ur = (ur.longitude, ur.latitude)\n",
    "ll = (ll.longitude, ll.latitude)\n",
    "extra = 0.01           # padding for edges\n",
    "coords = list(chain(ll, ur))\n",
    "w, h = coords[2] - coords[0], coords[3] - coords[1]\n",
    "\n",
    "m = Basemap(\n",
    "    projection='tmerc',\n",
    "    lon_0=-122.,\n",
    "    lat_0=37.,\n",
    "    ellps = 'WGS84',\n",
    "    llcrnrlon=coords[0] - extra * w,\n",
    "    llcrnrlat=coords[1] - extra * h,\n",
    "    urcrnrlon=coords[2] + extra * w,\n",
    "    urcrnrlat=coords[3] + extra * h,\n",
    "    lat_ts=0,\n",
    "    resolution='i',\n",
    "    suppress_ticks=True)\n",
    "\n",
    "m.readshapefile(\n",
    "    baseFile,\n",
    "    'oakland',\n",
    "    color='blue',\n",
    "    zorder=2)\n",
    "  \n",
    "# set up a map dataframe\n",
    "df_map = pd.DataFrame({\n",
    "    'poly': [Polygon(xy) for xy in m.oakland],\n",
    "    'id': [obj['OBJECTID'] for obj in m.oakland_info],\n",
    "    'zip': [obj['ZIP'] for obj in m.oakland_info],\n",
    "    'yearBuilt': [obj['YEARBUILT'] for obj in m.oakland_info]})\n",
    "\n",
    "# Create projection view as a polygon to filter shapes\n",
    "window = [ll, (ll[0], ur[1]), ur, (ur[0], ll[1]), ll]\n",
    "window = list(zip( *m(*list(zip(*window))) ))\n",
    "window_map = pd.DataFrame({'poly': [Polygon(window)]})\n",
    "window_polygon = prep(MultiPolygon(list(window_map['poly'].values)))\n",
    "\n",
    "# Remove any shapes that are outside the map window\n",
    "df_map = df_map[ [window_polygon.intersects(i) for i in df_map.poly] ]\n",
    "\n",
    "# draw tract patches from polygons\n",
    "df_map['patches'] = df_map['poly'].map(lambda x: PolygonPatch(\n",
    "    x,\n",
    "    ec='#787878', lw=.25, alpha=.9,\n",
    "    zorder=4))\n",
    "\n",
    "# create colormap based on year built\n",
    "cmap_range = (1885.5, 1930.5)\n",
    "ncolors = 8\n",
    "yearBuilt_bins = np.linspace(min(cmap_range), max(cmap_range), ncolors+1)\n",
    "cmap = matplotlib.cm.coolwarm\n",
    "cmap.set_bad(color='white')       # if yearBuilt is nan\n",
    "norm = matplotlib.colors.BoundaryNorm(yearBuilt_bins, ncolors)\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
    "\n",
    "# plot parcels by adding the PatchCollection to the axes instance\n",
    "pc = PatchCollection(df_map['patches'].values, match_original=True)\n",
    "pc.set_facecolor(cmap(norm(df_map.yearBuilt)/ncolors));\n",
    "ax.add_collection(pc)\n",
    "\n",
    "# create labels for colorbar\n",
    "yearBuilt_labels = ['%.0f-%.0f' % (yearBuilt_bins[i], yearBuilt_bins[i+1])\n",
    "                        for i in range(ncolors)]\n",
    "yearBuilt_labels.append('>%.0f' % yearBuilt_bins[-1])\n",
    "\n",
    "cb = colorbar_index(ncolors=ncolors+1, cmap=cmap, shrink=0.5, labels=yearBuilt_labels)\n",
    "cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "# Draw a map scale\n",
    "m.drawmapscale(\n",
    "    coords[0] + w * 0.5, coords[1] + h * 0.1,\n",
    "    coords[0], coords[1],\n",
    "    radius/2*1000,   # length\n",
    "    barstyle='fancy', labelstyle='simple',\n",
    "    units = 'm',\n",
    "#    format='%.2f',\n",
    "    fillcolor1='w', fillcolor2='#555555',\n",
    "    fontcolor='#555555',\n",
    "    zorder=4)\n",
    "plt.title(\"Oakland housing development, 1890-1930\")\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(7.22, 5.25)  \n",
    "plt.savefig('data/Oakland_temp.png', dpi=300, alpha=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping code above produces the following map:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_500m_1890to1930.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's easy to see a couple of trends in this small scale map:\n",
    "- Although roads aren't labeled, they are clearly visible in the negative space.  The big diagonal swoosh is I-580, and the almost horizontal road spanning the bottom is Park Blvd.  Even some pedestrian pathways, a common occurance in the hilly East Bay, are visible as thin lines between houses in the upper right.\n",
    "- Some parcels aren't labeled at all, not even with parcel boundaries.  These are not in Zillow's database, such as schools, churches, commercial property, and parks.  The big empty space on the right side is Oakland High School.\n",
    "- Some parcels have boundaries but are colored white.  This indicates they were either built after this date span or have no valid date of construction - overflow or invalid data, in other words.\n",
    "\n",
    "With that in mind, here are two larger scale maps containing the 6000 parcels closest to the centerpoint, for two slightly overlapping time scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_1300m_1880to1920.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_1300m_1920to1960.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the later map, all bright red parcels were built after 1960.\n",
    "\n",
    "Looking at these larger scale maps, a couple trends can be seen.  In the late 19th Century, or the Victorian Era, houses were built to the south and east of Lake Merritt, primarily in the Clinton and Bella Vista neighborhoods, with a few between Merrit and Cleveland Heights.  These houses tended to group in small numbers and be separated by a block or two: perhaps this area was still used for agriculture.  In any case, this area was then called Brooklyn and was separated from downtown Oakland by a toll bridge.\n",
    "\n",
    "The first two decades of the 20th C saw the edges of development spreading outward.  The development was strongest in the neighborhoods of Lakeshore, Cleveland Heights, and the upper elevations of Trestle Glen (to the SE of the where the label is).  These areas aligned closely with the building of the streetcar system, which by then ran through all these areas.\n",
    "\n",
    "Just about the only area to see complete development was Bella Vista, likely because its proximity to the Arbor Villa estate of Francis \"Borax\" Smith made it a very desirable location.  Oddly, Haddon Hill saw only a few houses being built before 1920 - perhaps this hill that overlooks Lake Merritt smelled too strong, for the lake (truly a tidal inlet then) was quite polluted.  The valley of Trestle Glen was almost ignored during this period: the train trestle that lent the name was torn down around 1906, but very few houses were built until 10-20 years later.\n",
    "\n",
    "Then in the 1920s most of these neighborhoods were almost completely developed, approaching 90% coverage.  The big exceptions are the Clinton area, which saw only 70-80% coverage, Arbor Villa, and the western slope of Haddon Hill.  Arbor Villa wasn't built until Francis Smith went bankrupt in the early 1930s and had to sell off his estate that spanned 5 city blocks.  The only thing remaining - sadly, his mansion was razed - is a row of palm trees on the southern edge that stretches 3 blocks long.  It wasn't until the Great Depression was truly over in 1940 that this area saw substatial development, as you can see in the plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_ArborVilla.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the 1950s, Arbor Villa been almost entirely filled in.  Given the quality of the dataset - to be discussed shortly - it's hard to make statements about specific parcels, which were the only ones built after this time.  An interesting question is how many were rebuilt because of catastrophe (fire or the 1989 Loma Prieta earthquake) versus changing use (converting a single family home to a multi-unit building, or residential to commercial).\n",
    "\n",
    "So my take-away from this analysis is that houses in these districts were not built in large tracts at the same time, as suburbs were in the 1950s, but rather piece-meal over a span of 5-15 years.  A couple questions arose: why was Clinton so sparsely developed?  It may be that it was originally quite dense, and that many buildings fell into disrepair and were replaced after the 1960s, or that it has always had more light industry than its surroundings.  Is there a trend for more recent buildings to be large apartment buildings?  I say this because many such buildings are in areas where there seem to be many more apartment buildings: Merritt, where Haddon Hill meets Lake Merritt, to the W of Grand Lake across the highway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that this dataset is from 2011 (parcel data) and 2016 (year built data), and does not include any structures that have been razed in the past (or previous parcels that have been subsequently subdivided).  Arbor Villa is the biggest example so far, but there were surely others.  Refining the dataset to include this data would be a substantial research project in itself (and would probably involve close reading of the Sanborn maps created for insurance quotes in the 19th and early 20th centuries, though they only came out every 5-10 years).\n",
    "\n",
    "How accurate is this data in the first place?  The Zillow information mostly comes from county sources - one can explore this on a property-by-property basis on their website.  Comparing a few cases with historical sources such as the [Oakland Wiki](https://localwiki.org/oakland \"Oakland - LocalWiki\") gives a sense of accuracy, or at least a closer approximation.  Here are some examples:\n",
    "\n",
    "- 2901 Park Blvd (corner of Park and McKinley) was built around 1912 (as part of the [Mary Smith Home for Friendless Girls](https://localwiki.org/oakland/Mary_Smith_Home_for_Friendless_Girls)), while Zillow puts it at 1930.\n",
    "- [1047 Bella Vista](https://oaklandwiki.org/Fenton_Home_Orphanage) was built in 1892, 18 years before Zillow claims it was.  (An interesting aside: Susan Fenton, the sister of the woman who founded Fenton's Creamery, which is still operating on Piedmont St and as beloved as ever, founded a Home for Destitue Children here in 1925.)\n",
    "- [The Kaiser house](https://oaklandwiki.org/Kaiser_House) at 664 Haddon Road, where Henry Kaiser lived between 1925 and the mid 1940s, was built in 1924.  Zillow cites a date of 1925.\n",
    "\n",
    "More historical citations can be found in [\"An Architectural Guidebook to San Francisco and the Bay Area\"](https://books.google.com/books?id=FkVQx6MWa8MC&lpg=RA2-PA120&ots=OANNWFMFSG&dq=%221047%20bella%20vista%22%20oakland&pg=RA2-PA120#v=onepage&q=%221047%20bella%20vista%22%20oakland&f=false), by Susan Dinkelspiel Cerny (2007).\n",
    "\n",
    "Based on this comparison, it seems like houses older than around 1920 are less likely to be accurately labeled in Zillow's archives than more recent houses.\n",
    "\n",
    "And finally, for reference, here is a plot of all parcels with an error, either because it's not in Zillow's database or because it's present but has an invalid date ('nan')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_2000m_errorParcels.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to extend this analysis:\n",
    "- Look at larger geographic areas, such as all of Oakland as well as surrounding communities.  The most prominent omission right now is Piedmont, a town which never incorporated into Oakland and is just barely visible as the straight edge at the top of the larger maps.  In particular, it would be interesting to look at the flats of Berkeley, Oakland, and Emeryville that were developed by people displaced by the 1906 earthquake in San Francisco.\n",
    "- Map other attributes.  The Zillow query returns several other interesting fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('zpid', '24763478'),\n",
       "             ('address',\n",
       "              OrderedDict([('street', '684 Spruce St'),\n",
       "                           ('zipcode', '94610'),\n",
       "                           ('city', 'Oakland'),\n",
       "                           ('state', 'CA'),\n",
       "                           ('latitude', '37.806119'),\n",
       "                           ('longitude', '-122.23984')])),\n",
       "             ('FIPScounty', '6001'),\n",
       "             ('useCode', 'MultiFamily2To4'),\n",
       "             ('taxAssessmentYear', '2015'),\n",
       "             ('taxAssessment', '204567.0'),\n",
       "             ('yearBuilt', '1910'),\n",
       "             ('lotSizeSqFt', '4400'),\n",
       "             ('finishedSqFt', '700'),\n",
       "             ('bathrooms', '1.0'),\n",
       "             ('bedrooms', '2'),\n",
       "             ('totalRooms', '8'),\n",
       "             ('lastSoldDate', '02/22/1996'),\n",
       "             ('lastSoldPrice',\n",
       "              OrderedDict([('@currency', 'USD'), ('#text', '148000')]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_queried[32.873555]['zillow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd expect many of these fields to show interesting patterns on this map.  The useCode would show which areas have most rental units compared to single-family homes, and would likely give a good sense of (residential) building height in each area.\n",
    "\n",
    "Total rooms and finished square feet may also be good proxies for how desirable the house was when built: presumably, larger houses were built for wealthier families.  (Of course, these numbers will reflect additions and renovations in the time since, and may not be as reliable as a result).\n",
    "\n",
    "Obviously, assessed value is a crucial variability that I have for the most part ignored, since there's already a large industry devoted to it.\n",
    "\n",
    "In order to follow up the observation about hilltops (in general) being developed first, the data set could also be expanded to incorporate elevation information.  This would then need to be filtered to find local maxima (hill tops), slopes (hill-sides), and local minima (valleys).  Based on the results here, I can easily imagine a weak but discernable correlation ($r=0.2-0.5$) between year built and location type, though I'm not sure this trend would be also be valid for neighborhoods such as the Oakland Hills that likely had different time courses of development.\n",
    "\n",
    "Lastly, please feel free to fork this code and apply it to your own neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPDATE with 40,000 properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a couple of months of running 1000 queries a day, I can now show a larger view of Oakland.  The 40,000 properties analyzed here is about five times more than the previous section (which had 7,500 properties).  The analysis now covers about 40% of the city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_6km_1895to2015.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this scale, the inadequecies of the data set start to become apparent: the town of Piedmont is a big empty hole (see note above).  It's clear the commercial districts have very poor coverage, such as downtown Oakland being a sea of white (about 3 km West from the center of the plot), which is only slightly smaller than Lake Merritt.  The problem again is that the Zillow database only has residential property: data for commercial properties would require a trip to City Hall.\n",
    "\n",
    "The most interesting new feature is the little patch of recent construction (red) on the periphery between noon and 1 o'clock.  These houses were rebuilt after the Oakland Hills Fire in 1991 (the Tunnel Fire), which destroyed almost 3000 structures and killed 25 and which remains the most destructive wild fire in California's history.  Here's a map of the fire (obtained [here](http://wildfiretoday.com/wp-content/uploads/2011/10/Oakland-Hills-fire-map.jpg)):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland-Hills-fire-map.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the map of the fire shows, just the southern edge of the fire is visible in the development map so far.  The thin strip of damage that went across Mountain View Cemetery isn't reflected, obviously, since no residential structures were affected.  It will be interesting to see the edges of the fire emerge on the development map, and to see how many houses in that area survived.\n",
    "\n",
    "Here are some more subtle features that can now be seen:\n",
    "- There was slower housing development in the hills, with some parcels being built decades after the rest of the neighborhood.  This is likely due to challenging terrain (steep slopes) on some plots, which was only appealing after the easier terrain had been built upon. \n",
    "- Point Adams, the neighborhood directly to the north of Lake Merritt, is very close to downtown and yet saw house construction over many decades.  I'd expect that an area so close to downtown would have been completely developed just after 1900.  Perhaps its proximity lent itself to redevelopment, where single-family homes were replaced with apartment complexes?  You can certainly see plenty of multi-family homes when driving through the neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPDATE with complete data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now about half a year after the initial post, and the data set has been completely queried.  It has 83,000 parcels with full information and 24,000 that threw an error for any of the reasons listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_8km_1985to2015.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this big overview, it's easy to see how the first development in the 1890s and 1900s was clustered in the neighborhoods of West Oakland, Clinton, and East Oakland around 14th Street and 90th Avenue.  The latter two areas were initially more spread out, presumably with larger lot sizes.  By the end of WWII, most of the flats were pretty much developed to their current density.\n",
    "\n",
    "More recently, the south-western hills around Merritt college have seen a lot of development.  It looks like a piece of land was rezoned for residential use, and a quick web search shows that this was indeed the case: Oakland [sold a portion of Merritt College](http://www.sfgate.com/news/article/Oakland-Finally-Sells-Old-Merritt-College-3044354.php) for development in 1995, using the proceeds to help maintain Merritt College's historic architecture.  Also, a surprising number of housing has been built in the Jack London area south of downtown: this seems to reflect the recent wave of reurbanization, where people are moving back to city centers.  And the punctate nature of the new development is consistent with condominiums being built on previously commercial lots.\n",
    "\n",
    "Let's look at the Tunnel Fire more closely now, since a large part of the periphery can be seen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_2km_TunnelFire.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My expectation of this fire was that it destroyed every house in its path.  This isn't the case here (assuming the data are true): about 1-2 houses per block survived destruction.  But note that the dataset only tells if the house was rebuilt.  It's possible that major damage could have been fixed with a permit short of a full rebuild.  This is veering off into real-estate definitions that I frankly don't know very well.\n",
    "\n",
    "What this map does show more clearly is that the bulk of the rebuilding happened within 7 years of the fire, and a substantial portion within 3 years.  It would be interesting to show a histogram of when these houses were rebuilt (among other kinds of statistics), but that would require a clean dataset of only those houses inside the fire perimeter that were known to have been built before 1991.  That kind of a historical dataset would be interesting for almost everything discussed so far, and doesn't exist online yet.  Already, there's a sense that not every plot in this area was developed when the fire came, since the area in the lower right has a smattering of parcels developed since 2007.  That density is about the same inside the fire perimeter, suggesting that both areas were developed to the same density in 1991.\n",
    "\n",
    "Moving on to an error analysis, the missing parcels are still salient, and best seen by looking at the map below.  The biggest concentrations are: the commercial area around 880 (the south-western edge of the city), the entrance to the Caldecott Tunnel (in the middle of the burned area), the port of Oakland to the West, Mills College, Merritt College, the Oakland Zoo, a golf course, an undeveloped hillside, Oakland International Airport, and several parcels that extend miles beyond the edges of this map into the San Francisco Bay (wrapping around the island township of Alameda that lies just south of downtown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_8km_errorParcels.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes my analysis for now.  I may revisit several of the open leads so far: analyzing Piedmont, plotting other kinds of data (single family vs. multi-family structures), maybe even the topographic effects of when an area was developed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
