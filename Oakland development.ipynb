{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development history of a few Oakland neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growing up in a small town on the East Coast, I knew the person who built our house.  Not personally, mind you, but I knew that he was the brother of the father of our next-door-neighbors (who were themselves quite old when I was a child).  Then I began to move around: first for college, then for graduate school, then to the Bay Area.  When I recently moved to Oakland and started putting down roots, I was missing the same kind of connection to my new neighborhood.  So I got a copy of Beth Bagwell's book [*Oakland: The Story of a City*](http://blog.ouroakland.net/2012/05/oakland-story-of-city.html), which filled in some broad background: first the Muwekma Ohlone villages and shell mounds, then the Spanish land grants to the rancheros (Peralta and his sons), then the gold rush and the land grab by greedy settlers, then the development of downtown and gradual spreading of the city boundary enabled by mass transit.  Then my neighborhood was built.  This was followed by WWII and the influx of African-Americans to work in the war effort, and then the grimmer recent history of the city center being abandoned for the suburbs.\n",
    "\n",
    "But somewhere in that narrative was my question, still unanswered: how was my neighborhood constructed?  Sure, one can find old maps that list the development parcels, but what did the neighborhood look like going up?  Was it built all at once in the roaring 20s, or piecemeal, one-farmhouse here, another there, as the land slowly became subdivided and the orchards and oak trees cut down?  This is a narrow question, and more recent history is layered on top of this: how the highways divided many neighbors, how new apartment buildings went up, structures that burned down or were destroyed.  And it also ignores how the land was used by the Peraltas for decades, and the Ohlone for many centuries before them.  But the further back one looks, the fainter the traces.  Or, said another way: you have to start somewhere.\n",
    "\n",
    "So I decided to answer the simplest question I could; when was each house built?  This required some information that shouldn't be too hard to find: a list of houses, where each one is, and when it was built.  I'll describe the process of analyzing this data as I go along, complete with snippets in case others want to replicate this info.  Standalone scripts can be found in this repository as well, as indicated in comments.\n",
    "\n",
    "To skip right to the analysis, scroll down to the maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data sets made available as part of the [CodeForOakland](http://codeforoakland.org/data-sets/#oakland1) project have been crucial.  Initially, I found a GeoJSON file containing parcel info for the county of Alameda, which includes Oakland but includes several other large cities.  This was a huge file with 10x more data than just Oakland, couldn't be processed in memory, and lacked street addresses.  Several days later, I found a shapefile of all parcels in Oakland, from around 2011, that conveniently fit into memory - this was exactly what was needed.  Still, if I want to go back and look at the info for Piedmont (or another neighboring city), I can always dig up the county database.\n",
    "\n",
    "This is the info for each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels 105351\n",
      "\n",
      "Second parcel:\n",
      "{'geometry': {'coordinates': [[(-122.244024425766, 37.86867162821933),\n",
      "                               (-122.24398667467038, 37.86866162294282),\n",
      "                               (-122.24399852865695, 37.868349112878676),\n",
      "                               (-122.24401355395213, 37.86835277617628),\n",
      "                               (-122.24404457097216, 37.868359870908144),\n",
      "                               (-122.2440757393451, 37.86836653553029),\n",
      "                               (-122.24408444864746, 37.86836826921976),\n",
      "                               (-122.24407290434733, 37.86867278199862),\n",
      "                               (-122.244024425766, 37.86867162821933)]],\n",
      "              'type': 'Polygon'},\n",
      " 'id': '1',\n",
      " 'properties': OrderedDict([('OBJECTID', 23),\n",
      "                            ('ADDR_HN', None),\n",
      "                            ('ADDR_PD', None),\n",
      "                            ('ADDR_SN', 'DWIGHT'),\n",
      "                            ('ADDR_ST', 'WAY'),\n",
      "                            ('OBJECTID_1', 116),\n",
      "                            ('APN', '048H770309700'),\n",
      "                            ('ADDRNUM', 0),\n",
      "                            ('STNAME', 'DWIGHT WY'),\n",
      "                            ('UNIT', None),\n",
      "                            ('ZIP', 94704),\n",
      "                            ('DATASRC', 'ASSR_20031023'),\n",
      "                            ('ORIGKEY', '048H770309700'),\n",
      "                            ('ADDRESS_AP', '048H770309700'),\n",
      "                            ('APNSRC', None),\n",
      "                            ('ADDRNUM_TX', None),\n",
      "                            ('SHAPE_AREA', 2830.19306092),\n",
      "                            ('SHAPE_LEN', 276.033464863)]),\n",
      " 'type': 'Feature'}\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import pprint\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "source = fiona.open(baseFile + '.shp')\n",
    "print('Number of parcels: %d\\n' % len(source))\n",
    "print('Second parcel:')\n",
    "source.next()    # first entry has a lot of coordinates, so skip\n",
    "pprint.pprint(source.next())\n",
    "\n",
    "source.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this database has a field for the street address, though the specific example above lacks this information ('None').  I'll catch this kind of error below.  If the street address wasn't present, it could be found by reverse geocoding the location (from Google's API, for instance).  All told, there are 105,000 entries.\n",
    "\n",
    "The construction history exists in Zillow's database, so the dataset was completed by calling Zillow's API for each property.  The one catch is that Zillow's free API key allows only 1,000 queries per day.  Because I'm mostly interested in a few neighborhoods, I can approach this rate limit in a smart way by working outward from a central point; radial processing, if you will.  I'll chooose the center location to be a landmark in my neighborhood of Cleveland Heights: the Armenian Church on the top of the hill, at the corner of McKinley and Spruce.  At this rate, it will take 105 days (~3.5 months) to process all the data for Oakland.  All the surrounding neighborhoods should take a couple of weeks at most, which is a much more realistic proposition.\n",
    "\n",
    "To process the parcels radially, the shapefile array was rewritten as a dictionary with the key being the distance between the landmark and the centroid of each parcel.  Notice the data stays as a dictionary to facilitate saving it back to a shape file at the end, which is useful for mapping purposes.  If that wasn't a concern, it would make a lot of sense to convert the format to something easier to manipulate, like a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parcels in dict: 97718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fiona\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "import shapely.geometry as shp\n",
    "import pickle \n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'\n",
    "center = (37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "\n",
    "data_raw = {}\n",
    "data_duplicates = {}\n",
    "\n",
    "with fiona.drivers():           # Register format drivers with a context manager\n",
    "\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "       \n",
    "        for f in source :\n",
    "            if 'geometry' not in f:\n",
    "                print('No geometry key in entry {}'.format(f))\n",
    "            c = shp.shape(f['geometry']).centroid\n",
    "            p = (c.y, c.x)\n",
    "\n",
    "            d = round(distance(p, center).m * 10**6)/10**6        # round to micrometers\n",
    "            f['centroid'] = p            \n",
    "\n",
    "            if d in data_raw :\n",
    "                if d in data_duplicates :   \n",
    "                    data_duplicates[d].append(f)      # add to list in existing dictionary key\n",
    "                else :\n",
    "                    data_duplicates[d] = [data_raw[d], f]      # create list in existing dictionary key\n",
    "            else :\n",
    "                data_raw[d] = f\n",
    "print('Number of parcels in dict: %d\\n' % len(data_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, computing the centroid is unnecessary for radial processing - the first coordinate of the parcel shape would suffice - but this info may be useful later on.  Also, the amount of time it adds to processing is negligible for such a small dataset.\n",
    "\n",
    "Notice that there are now about 97,000 entries, about 8,000 less than the original file.  It turns out that these are duplicate entries, such as condominiums, that share the same street address and coordinates but have different assessor parcel numbers (APNs).  Doing the math confirms that this accounts for all duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of duplicate parcels: 7633\n"
     ]
    }
   ],
   "source": [
    "duplicateCount = 0\n",
    "for (key, value) in data_duplicates.items() :\n",
    "    duplicateCount += len(value)\n",
    "print('Total number of duplicate parcels: %d' % (duplicateCount-len(data_duplicates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$7633+97718 = 105351$, which was the original number of entries in the shapefile.\n",
    "\n",
    "Interestingly, the micrometer precision in the distance key is necessary to distinguish parcels: rounding to millimeters results in clashes between different street addresses.  Given the number of parcels at a certain radius once the radius gets large, this isn't terribly surprising, but still an nice example about the importance of precision and probability.\n",
    "\n",
    "Now the cleaned dataset is saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/OaklandParcels_inProcess.pkl', 'wb') as datafile :\n",
    "    a = pickle.Pickler(datafile)\n",
    "    compressed = {}\n",
    "    compressed['data_raw'] = data_raw\n",
    "    compressed['data_queried'] = {}\n",
    "    compressed['data_errors'] = []\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is processed starting with the smallest key and working outwards: each entry is first sent to the Zillow API, then popped from the input dictionary (data_raw) and placed in the output dictionary (data_queried) if the response is valid.  If the response is invalid, it is placed into an error dictionary (data_errors) for later processing.  There's also a rate limit of 10 queries/second, so a timer around the loop limits the rate.  It runs at 5 queries/second just to be nice to Zillow's server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CreateParcelDatabase.py\n",
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Zillow variables keys\n",
    "with open('../private/API_keys.pkl', 'rb') as datafile:\n",
    "    zid = pickle.load(datafile)             # API key\n",
    "zurl = 'http://www.zillow.com/webservice/GetDeepSearchResults.htm?'\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'\n",
    "radius = 1000            # only process parcels within this radius - used for debugging\n",
    "numToProcess = 1000      # zillow API limits to 1000 queries per day\n",
    "\n",
    "# load data structures\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "data_errors = compressed['data_errors']\n",
    "del compressed\n",
    "\n",
    "# sort keys by distance from closest to furthest\n",
    "sortedKeys = [k for k in sorted(data_raw) if k < radius]\n",
    "\n",
    "for (i, key) in zip(range(numToProcess), sortedKeys) :\n",
    "    startT = time.time()            # set up timer to keep requests under 10/s\n",
    "\n",
    "    try :\n",
    "        # read in address details from input dictionary\n",
    "        zp = {'address' : '{} {} {}'.format(data_raw[key]['properties']['ADDR_HN'],\n",
    "                      data_raw[key]['properties']['ADDR_SN'],\n",
    "                      data_raw[key]['properties']['ADDR_ST']),\n",
    "              'citystatezip' : 'Oakland, CA ' + str(data_raw[key]['properties']['ZIP']),\n",
    "              'zws-id' : zid}\n",
    "        r = requests.get(zurl, params=zp)\n",
    "        r_dict = xmltodict.parse(r.text)['SearchResults:searchresults']\n",
    "\n",
    "        if r_dict['message']['code'] == '0' :       # valid response?\n",
    "            r_dict = r_dict['response']['results']['result']\n",
    "            \n",
    "            # in case the response is a list of multiple (similar) entries, take the first one\n",
    "            if type(r_dict)==list :\n",
    "                r_dict = r_dict[0]\n",
    "            # prune extraneous fields\n",
    "            r_dict.pop('links')\n",
    "            r_dict.pop('zestimate')\n",
    "            r_dict.pop('localRealEstate')\n",
    "            data_raw[key]['zillow'] = r_dict\n",
    "            # transfer  to output dictionary\n",
    "            data_queried[key] = data_raw.pop(key)\n",
    "            \n",
    "        else :\n",
    "            print('For request {}, zillow code is {}. Here''s the record:'.format(\n",
    "                    zp['address'], r_dict['message']['code']))\n",
    "            print(r_dict)\n",
    "            print('-'*60)\n",
    "            # transfer info to error dictionary for offline analysis\n",
    "            data_errors.append({'key': key, 'value': data_raw.pop(key), \n",
    "                                  'zillow': r_dict, 'source': 'zillow'})\n",
    "    except Exception as exc:\n",
    "        print('Unspecified error: {}'.format(exc))\n",
    "        data_errors.append({'key': key, 'value': data_raw.pop(key),\n",
    "                              'source': 'exception'})\n",
    "            \n",
    "    # log status\n",
    "    print(i, ' ', zp['address'])\n",
    "\n",
    "    endT = time.time()\n",
    "    if endT - startT < 0.2 :\n",
    "        time.sleep(0.2 - (endT-startT))     # rate limit to 5 calls per second\n",
    "\n",
    "# save dictionaries back to disk\n",
    "compressed = {}\n",
    "compressed['data_raw'] = data_raw\n",
    "compressed['data_queried'] = data_queried\n",
    "compressed['data_errors'] = data_errors\n",
    "with open(inProcessFile, 'wb') as fid :\n",
    "    a = pickle.Pickler(fid)\n",
    "    a.dump(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common error was code 508, 'no exact match found for input address'.  This was primarily caused by the address not being in the Zillow database, such as for commercial buildings, churches, schools, etc.  But this was also caused by invalid address, such as a parcel without a street address ('None MacArthur Blvd'), which was the case for parks, municipal land, and Lake Merritt.  The error rate was about 5%, or 1 in 20.\n",
    "\n",
    "Before mapping the data, it needs to be written back to a shapefile so it can be easily processed.  Even though shapefiles are an old format, they are pretty efficient for this kind of processing - much more so than the GeoJSON format.  When doing so, the schema from the original file needs to be modified to include the new data ('yearBuilt').  Extra fields from Zillow are jettisoned to speed up processing.  Finally, because this shapefile has the 'properties' dictionary in an OrderedDict class, the data fields need to be rearranged to match the schema's order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SaveParcelDictionaryAsShapefile.py\n",
    "import fiona\n",
    "import pickle\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "inProcessFile = 'data/OaklandParcels_inProcess.pkl'         # data source\n",
    "with open(inProcessFile, 'rb') as fid :\n",
    "    compressed = pickle.load(fid)\n",
    "# ...and unpack\n",
    "data_raw = compressed['data_raw']\n",
    "data_queried = compressed['data_queried']\n",
    "del compressed\n",
    "\n",
    "baseFile = 'data/Oakland_parcels/parcels'       # source of shape info in dictionary\n",
    "outputFileName = 'Oakland_parcels_queried'\n",
    "radius = 2000                                   # in m\n",
    "\n",
    "# create output directory if it doesn't exist yet\n",
    "if os.path.isdir('data/' + outputFileName) is False :\n",
    "    os.makedirs('data/' + outputFileName)\n",
    "outputFile = 'data/' + outputFileName + '/' + outputFileName + '.shp'\n",
    "\n",
    "# Register format drivers with a context manager\n",
    "with fiona.drivers():\n",
    "    # get schema from original file\n",
    "    with fiona.open(baseFile + '.shp') as source:\n",
    "        meta = source.meta\n",
    "        \n",
    "    # add new fields to schema file\n",
    "    meta['schema']['centroid'] = ('float:19:11', 'float:19:11')\n",
    "    meta['schema']['id'] = 'float:19'\n",
    "    meta['schema']['type'] = 'str:50'\n",
    "    meta['schema']['yearBuilt'] = 'float:10'\n",
    "    meta['schema']['properties']['YEARBUILT'] = 'int:6'\n",
    "    meta['schema']['properties'] = OrderedDict(meta['schema']['properties'])\n",
    "    schemaOrder = meta['schema']['properties']\n",
    "\n",
    "    with fiona.open(outputFile, 'w', **meta) as sink:\n",
    "        for (i,f) in enumerate(data_queried) :\n",
    "            if f <= radius :   \n",
    "                if 'yearBuilt' in data_queried[f]['zillow'] :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = data_queried[f]['zillow']['yearBuilt']\n",
    "                    data_queried[f].pop('zillow')\n",
    "                else :\n",
    "                    data_queried[f]['properties']['YEARBUILT'] = np.nan\n",
    "                # reorder dictionary to match schema order\n",
    "                data_queried[f]['properties'] = OrderedDict(\n",
    "                        (k, data_queried[f]['properties'][k]) for k in schemaOrder)           \n",
    "                sink.write(data_queried[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kind of map used to show this data is a choropleth map, which maps a physical quantity (year built) onto a spatial extent by using some kind of shading.  This is a quick way to show what spatial patterns might require more analysis.  While Python has no straightforward way to do this, all the tools required are free and there's extensive support and code examples online.\n",
    "\n",
    "The general overview is to first choose a projection grid to map the round world onto.  This converts the (longitude, latitude) pairs to (x, y) pairs in the coordinate system of the projection.  The parcel polygons are drawn and colored on top of this.  First, though, here are two functions to make colorbars, borrowed from [Sensitive Cities](http://sensitivecities.com/so-youd-like-to-make-a-map-using-python-EN.html#.V2hnJa4tVVz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convenience functions for working with colour ramps and bars\n",
    "def colorbar_index(ncolors, cmap, labels=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This is a convenience function to stop you making off-by-one errors\n",
    "    Takes a standard colour ramp, and discretizes it,\n",
    "    then draws a colour bar with correctly aligned labels\n",
    "    \"\"\"\n",
    "    cmap = cmap_discretize(cmap, ncolors)\n",
    "    mappable = cm.ScalarMappable(cmap=cmap)\n",
    "    mappable.set_array([])\n",
    "    mappable.set_clim(-0.5, ncolors+0.5)\n",
    "    colorbar = plt.colorbar(mappable, **kwargs)\n",
    "    colorbar.set_ticks(np.linspace(0, ncolors, ncolors))\n",
    "    colorbar.set_ticklabels(range(ncolors))\n",
    "    if labels:\n",
    "        colorbar.set_ticklabels(labels)\n",
    "    return colorbar\n",
    "\n",
    "def cmap_discretize(cmap, N):\n",
    "    \"\"\"\n",
    "    Return a discrete colormap from the continuous colormap cmap.\n",
    "\n",
    "        cmap: colormap instance, eg. cm.jet. \n",
    "        N: number of colors.\n",
    "\n",
    "    Example\n",
    "        x = resize(arange(100), (5,100))\n",
    "        djet = cmap_discretize(cm.jet, 5)\n",
    "        imshow(x, cmap=djet)\n",
    "\n",
    "    \"\"\"\n",
    "    if type(cmap) == str:\n",
    "        cmap = get_cmap(cmap)\n",
    "    colors_i = np.concatenate((np.linspace(0, 1., N), (0., 0., 0., 0.)))\n",
    "    colors_rgba = cmap(colors_i)\n",
    "    indices = np.linspace(0, 1., N + 1)\n",
    "    cdict = {}\n",
    "    for ki, key in enumerate(('red', 'green', 'blue')):\n",
    "        cdict[key] = [(indices[i], colors_rgba[i - 1, ki], colors_rgba[i, ki]) for i in xrange(N + 1)]\n",
    "    return matplotlib.colors.LinearSegmentedColormap(cmap.name + \"_%d\" % N, cdict, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the mapping routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DrawParcelChoropleth.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "from descartes import PolygonPatch\n",
    "from itertools import chain\n",
    "import geopy.distance\n",
    "distance = geopy.distance.vincenty    \n",
    "\n",
    "# shapefile database\n",
    "baseFile = 'data/Oakland_parcels_queried/Oakland_parcels_queried'\n",
    "\n",
    "center = geopy.Point(37.8058428, -122.2399758)        # (lat, long), Armenian Church\n",
    "radius = 0.6                           # in km\n",
    "ur = distance(kilometers=radius*2**0.5).destination(center, +45)\n",
    "ll = distance(kilometers=radius*2**0.5).destination(center, -135)\n",
    "ur = (ur.longitude, ur.latitude)\n",
    "ll = (ll.longitude, ll.latitude)\n",
    "extra = 0.01           # padding for edges\n",
    "coords = list(chain(ll, ur))\n",
    "w, h = coords[2] - coords[0], coords[3] - coords[1]\n",
    "\n",
    "m = Basemap(\n",
    "    projection='tmerc',\n",
    "    lon_0=-122.,\n",
    "    lat_0=37.,\n",
    "    ellps = 'WGS84',\n",
    "    llcrnrlon=coords[0] - extra * w,\n",
    "    llcrnrlat=coords[1] - extra * h,\n",
    "    urcrnrlon=coords[2] + extra * w,\n",
    "    urcrnrlat=coords[3] + extra * h,\n",
    "    lat_ts=0,\n",
    "    resolution='i',\n",
    "    suppress_ticks=True)\n",
    "\n",
    "m.readshapefile(\n",
    "    baseFile,\n",
    "    'oakland',\n",
    "    color='blue',\n",
    "    zorder=2)\n",
    "  \n",
    "# set up a map dataframe\n",
    "df_map = pd.DataFrame({\n",
    "    'poly': [Polygon(xy) for xy in m.oakland],\n",
    "    'id': [obj['OBJECTID'] for obj in m.oakland_info],\n",
    "    'zip': [obj['ZIP'] for obj in m.oakland_info],\n",
    "    'yearBuilt': [obj['YEARBUILT'] for obj in m.oakland_info]})\n",
    "\n",
    "# Create projection view as a polygon to filter shapes\n",
    "window = [ll, (ll[0], ur[1]), ur, (ur[0], ll[1]), ll]\n",
    "window = list(zip( *m(*list(zip(*window))) ))\n",
    "window_map = pd.DataFrame({'poly': [Polygon(window)]})\n",
    "window_polygon = prep(MultiPolygon(list(window_map['poly'].values)))\n",
    "\n",
    "# Remove any shapes that are outside the map window\n",
    "df_map = df_map[ [window_polygon.intersects(i) for i in df_map.poly] ]\n",
    "\n",
    "# draw tract patches from polygons\n",
    "df_map['patches'] = df_map['poly'].map(lambda x: PolygonPatch(\n",
    "    x,\n",
    "    ec='#787878', lw=.25, alpha=.9,\n",
    "    zorder=4))\n",
    "\n",
    "# create colormap based on year built\n",
    "cmap_range = (1885.5, 1930.5)\n",
    "ncolors = 8\n",
    "yearBuilt_bins = np.linspace(min(cmap_range), max(cmap_range), ncolors+1)\n",
    "cmap = matplotlib.cm.coolwarm\n",
    "cmap.set_bad(color='white')       # if yearBuilt is nan\n",
    "norm = matplotlib.colors.BoundaryNorm(yearBuilt_bins, ncolors)\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
    "\n",
    "# plot parcels by adding the PatchCollection to the axes instance\n",
    "pc = PatchCollection(df_map['patches'].values, match_original=True)\n",
    "pc.set_facecolor(cmap(norm(df_map.yearBuilt)/ncolors));\n",
    "ax.add_collection(pc)\n",
    "\n",
    "# create labels for colorbar\n",
    "yearBuilt_labels = ['%.0f-%.0f' % (yearBuilt_bins[i], yearBuilt_bins[i+1])\n",
    "                        for i in range(ncolors)]\n",
    "yearBuilt_labels.append('>%.0f' % yearBuilt_bins[-1])\n",
    "\n",
    "cb = colorbar_index(ncolors=ncolors+1, cmap=cmap, shrink=0.5, labels=yearBuilt_labels)\n",
    "cb.ax.tick_params(labelsize=6)\n",
    "\n",
    "# Draw a map scale\n",
    "m.drawmapscale(\n",
    "    coords[0] + w * 0.5, coords[1] + h * 0.1,\n",
    "    coords[0], coords[1],\n",
    "    radius/2*1000,   # length\n",
    "    barstyle='fancy', labelstyle='simple',\n",
    "    units = 'm',\n",
    "#    format='%.2f',\n",
    "    fillcolor1='w', fillcolor2='#555555',\n",
    "    fontcolor='#555555',\n",
    "    zorder=4)\n",
    "plt.title(\"Oakland housing development, 1890-1930\")\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(7.22, 5.25)  \n",
    "plt.savefig('data/Oakland_temp.png', dpi=300, alpha=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping code above produces the following map:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_500m_1890to1930.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's easy to see a couple of trends in this small scale map:\n",
    "- Although roads aren't labeled, they are clearly visible in the negative space.  The big diagonal swoosh is I-580, and the almost horizontal road spanning the bottom is Park Blvd.  Even some pedestrian pathways, a common occurance in the hilly East Bay, are visible as thin lines between houses in the upper right.\n",
    "- Some parcels aren't labeled at all, not even with parcel boundaries.  These are not in Zillow's database, such as schools, churches, commercial property, and parks.  The big empty space on the right side is Oakland High School.\n",
    "- Some parcels have boundaries but are colored white.  This indicates they were either built after this date span or have no valid date of construction - overflow or invalid data, in other words.\n",
    "\n",
    "With that in mind, here are two larger scale maps containing the 6000 parcels closest to the centerpoint, for two slightly overlapping time scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_1300m_1880to1920.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_1300m_1920to1960.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the later map, all bright red parcels were built after 1960.\n",
    "\n",
    "Looking at these larger scale maps, a couple trends can be seen.  In the late 19th Century, or the Victorian Era, houses were built to the south and east of Lake Merritt, primarily in the Clinton and Bella Vista neighborhoods, with a few between Merritt and Cleveland Heights.  These houses tended to group in small numbers and be separated by a block or two: perhaps this area was still used for agriculture?  In any case, this area used to be called Brooklyn and was separated from downtown Oakland by a toll bridge.\n",
    "\n",
    "The first two decades of the 20th C saw the edges of development spreading outward.  The development was strongest in the neighborhoods of Lakeshore, Cleveland Heights, and the upper elevations of Trestle Glen (to the SE of the where the label is).  These areas aligned closely with the building of the streetcar system, which by then ran through all these areas.\n",
    "\n",
    "Just about the only area to see complete development was Bella Vista, likely because its proximity to the Arbor Villa estate of Francis \"Borax\" Smith made it a very desirable location.  Oddly, Haddon Hill saw only a few houses being built before 1920 - perhaps this hill that overlooks Lake Merritt smelled too strong, for the lake (truly a tidal inlet then) was quite polluted.  The valley of Trestle Glen was almost ignored during this period: the train trestle that lent the name was torn down around 1906, but very few houses were built until 10-20 years later.\n",
    "\n",
    "Then in the 1920s most of these neighborhoods were almost completely developed, approaching 90% coverage.  The big exceptions are the Clinton area, which saw only 70-80% coverage, Arbor Villa, and the western slope of Haddon Hill.  Arbor Villa wasn't built until Francis Smith went bankrupt in the early 1930s and had to sell off his estate that spanned 5 city blocks.  The only thing remaining - sadly, his mansion was razed - is a row of palm trees on the southern edge that stretches 3 blocks long.  It wasn't until the Great Depression was truly over in 1940 that this area saw substantial development, as you can see in the plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_ArborVilla.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the 1950s, Arbor Villa been almost entirely filled in.  Given the quality of the dataset - to be discussed shortly - it's hard to make statements about specific parcels, which were the only ones built after this time.  An interesting question is how many were rebuilt because of catastrophe (fire or the 1989 Loma Prieta earthquake), versus changing use (converting a single family home to a multi-unit building, or residential to commercial), versus homeowner whim.\n",
    "\n",
    "So my take-away from this analysis is that houses in these districts were not built in large tracts at the same time, as suburbs were in the 1950s, but rather piece-meal over a span of 5-15 years.  A couple questions arose: why was Clinton so sparsely developed early on?  It may be that it was fully developed, but that many buildings were replaced after the 1960s.  There are also a fair number of error parcels in this district (see below), which indicate both mixed commercial use and omis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that this dataset is from 2011 or earlier, and does not include any structures that have been razed in the past (or previous parcels that have been subsequently subdivided).  Arbor Villa is the most famous example, but there were surely others.  Refining the dataset to include this data would be a substantial research project in itself (and would probably involve close reading of the Sanborn maps created for insurance quotes in the 19th and early 20th centuries, though they only came out every 5-10 years).\n",
    "\n",
    "How accurate is this data in the first place?  The Zillow information mostly comes from county sources - one can explore this on a property-by-property basis on their website.  Comparing a few cases with historical sources such as the [Oakland Wiki](https://localwiki.org/oakland \"Oakland - LocalWiki\") gives a sense of accuracy, or at least a closer approximation.  Here are some examples:\n",
    "\n",
    "- 2901 Park Blvd (corner of Park and McKinley) was built around 1912 (as part of the [Mary Smith Home for Friendless Girls](https://localwiki.org/oakland/Mary_Smith_Home_for_Friendless_Girls)), while Zillow puts it at 1930.\n",
    "- [1047 Bella Vista](https://oaklandwiki.org/Fenton_Home_Orphanage) was built in 1892, 18 years before Zillow claims it was.  (An interesting aside: Susan Fenton, the sister of the woman who founded Fenton's Creamery, which is still operating on Piedmont St and as beloved as ever, founded a Home for Destitue Children here in 1925.)\n",
    "- [The Kaiser house](https://oaklandwiki.org/Kaiser_House) at 664 Haddon Road, where Henry Kaiser lived between 1925 and the mid 1940s, was built in 1924.  Zillow cites a date of 1925.\n",
    "\n",
    "More historical citations can be found in [\"An Architectural Guidebook to San Francisco and the Bay Area\"](https://books.google.com/books?id=FkVQx6MWa8MC&lpg=RA2-PA120&ots=OANNWFMFSG&dq=%221047%20bella%20vista%22%20oakland&pg=RA2-PA120#v=onepage&q=%221047%20bella%20vista%22%20oakland&f=false), by Susan Dinkelspiel Cerny (2007).\n",
    "\n",
    "Based on this comparison, it seems like houses older than around 1920 are less likely to be accurately labeled in Zillow's archives than more recent houses.\n",
    "\n",
    "And finally, for reference, here is a plot of all parcels with an error, either because it's not in Zillow's database or because it's present but has an invalid date ('nan')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Oakland_2000m_errorParcels.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to extend this analysis:\n",
    "- Look at a larger area geographically, such as all of Oakland as well as surrounding communities.  The most prominent omission right now is Piedmont, a town which never incorporated into Oakland and is just barely visible as the straight edge at the top of the larger maps.  In particular, it would be interesting to look at the flats of Berkeley, Oakland, and Emeryville that were developed by people displaced by the 1906 earthquake in San Francisco.\n",
    "- Map other attributes.  The Zillow query returns several other interesting fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('zpid', '24763478'),\n",
       "             ('address',\n",
       "              OrderedDict([('street', '684 Spruce St'),\n",
       "                           ('zipcode', '94610'),\n",
       "                           ('city', 'Oakland'),\n",
       "                           ('state', 'CA'),\n",
       "                           ('latitude', '37.806119'),\n",
       "                           ('longitude', '-122.23984')])),\n",
       "             ('FIPScounty', '6001'),\n",
       "             ('useCode', 'MultiFamily2To4'),\n",
       "             ('taxAssessmentYear', '2015'),\n",
       "             ('taxAssessment', '204567.0'),\n",
       "             ('yearBuilt', '1910'),\n",
       "             ('lotSizeSqFt', '4400'),\n",
       "             ('finishedSqFt', '700'),\n",
       "             ('bathrooms', '1.0'),\n",
       "             ('bedrooms', '2'),\n",
       "             ('totalRooms', '8'),\n",
       "             ('lastSoldDate', '02/22/1996'),\n",
       "             ('lastSoldPrice',\n",
       "              OrderedDict([('@currency', 'USD'), ('#text', '148000')]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_queried[32.873555]['zillow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd expect many of these fields to show interesting patterns on this map.  The useCode would show which areas have most rental units compared to single-family homes, and would likely give a good sense of (residential) building height in each area.\n",
    "\n",
    "Total rooms and finished square feet may also be good proxies for how desirable the house was when built: presumably, larger houses were built for wealthier families.  (Of course, these numbers will reflect additions and renovations in the time since, and may not be as reliable as a result).\n",
    "\n",
    "Obviously, assessed value is a crucial variability that I have for the most part ignored, since there's already a large industry devoted to that question.\n",
    "\n",
    "In order to follow up the observation about hilltops (in general) being developed first, the data set could also be expanded to incorporate elevation information.  This would then need to be filtered to find local maxima (hill tops), slopes (hill-sides), and local minima (valleys).  Based on the results here, I can easily imagine a weak but discernable correlation ($r=0.2-0.5$) between year built and location type, though I'm not sure this trend would be also be valid for neighborhoods such as the Oakland Hills that likely had different time courses of development.\n",
    "\n",
    "Lastly, please feel free to fork this code and play around with it in your own neighborhood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
